{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data_preprocessed_cut_2_Valence.csv`と`data_preprocessed_cut_2_Arousal.csv`をtrainする  \n",
    "\n",
    "引数でdictを受け取るver\n",
    "devで相関係数を出す\n",
    "\n",
    "cross-validationをやる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.test.utils import datapath, get_tmpfile\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# # transform : glove -> tmp\n",
    "# glove2word2vec('./stanford_glove.txt', './stanford_w2v.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "gensim_model = gensim.models.KeyedVectors.load_word2vec_format('./stanford_w2v.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = gensim_model.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 2196016\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = weights.shape[1]\n",
    "print(embedding_dim,weights.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.append(weights,np.zeros((1,embedding_dim)),axis=0)\n",
    "# 末尾にunknown_wordを追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196017\n"
     ]
    }
   ],
   "source": [
    "vocab_size = weights.shape[0]\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq):\n",
    "    vocab = gensim_model.wv.vocab\n",
    "    idxs = [vocab[w].index if w in vocab else vocab_size - 1 for w in seq]\n",
    "    res = torch.tensor(idxs, dtype=torch.long)\n",
    "    if cuda:\n",
    "        res = res.cuda()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vec(sentence):\n",
    "    w_list = sentence.split()\n",
    "    res_seq = prepare_sequence(w_list)\n",
    "    return res_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = \"I'm always fucking you.\"\n",
    "# sentence2vec(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, option, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = option['hidden_size']\n",
    "        self.num_layers = option['num_layers']\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.bi = (2 if option['bidirectional'] else 1)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        option['input_size'] = embedding_dim\n",
    "        option['batch_first'] = True\n",
    "\n",
    "        self.lstm = nn.LSTM(**option)\n",
    "        \n",
    "        # 2layer固定 調べるのめんどい\n",
    "        self.lstm.weight_hh_l0.data.uniform_(-0.01,0.01)        \n",
    "#         self.lstm.weight_hh_l1.data.uniform_(-0.01,0.01)        \n",
    "        self.lstm.weight_ih_l0.data.uniform_(-0.01,0.01)        \n",
    "#         self.lstm.weight_ih_l1.data.uniform_(-0.01,0.01)       \n",
    "        \n",
    "#         self.lstm.weight_hh_l0_reverse.data.uniform_(-0.01,0.01)        \n",
    "#         self.lstm.weight_hh_l1_reverse.data.uniform_(-0.01,0.01)        \n",
    "#         self.lstm.weight_ih_l0_reverse.data.uniform_(-0.01,0.01)        \n",
    "#         self.lstm.weight_ih_l1_reverse.data.uniform_(-0.01,0.01)       \n",
    "        \n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.out = nn.Linear(self.hidden_dim*self.bi, tagset_size)\n",
    "        self.out.weight.data.uniform_(-0.01,0.01)\n",
    "\n",
    "        self.hidden = self.init_hidden(1)\n",
    "\n",
    "    def init_hidden(self,batch_size):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        h = torch.zeros(self.num_layers*self.bi, batch_size, self.hidden_dim)\n",
    "        c = torch.zeros(self.num_layers*self.bi, batch_size, self.hidden_dim)\n",
    "        if cuda:\n",
    "            h = h.cuda()\n",
    "            c = c.cuda()\n",
    "        return (h,c)\n",
    "\n",
    "    def forward(self, sentence, lengths):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "#         print(embeds.size())\n",
    "#         batch_size = embeds.size()[0]\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embeds,lengths,batch_first=True)\n",
    "        lstm_output, self.hidden = self.lstm(packed, self.hidden)\n",
    "        unpacked,_ = nn.utils.rnn.pad_packed_sequence(lstm_output,batch_first=True)\n",
    "        # print(unpacked.size())\n",
    "        # :batch * max(len(lengths)) * hidden\n",
    "        \n",
    "        unpacked = torch.mean(unpacked,1)\n",
    "        # print(unpacked.size())\n",
    "        # :batch * hidden\n",
    "        output = self.out(unpacked)\n",
    "        output = F.tanh(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(option):\n",
    "    # 学習済みパラメータ\n",
    "    torch.manual_seed(2)\n",
    "    model = LSTMTagger(embedding_dim, option, vocab_size, out_size)\n",
    "    model.word_embeddings = nn.Embedding.from_pretrained(torch.from_numpy(weights).float())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h = torch.nn.LSTM(1,2,1)\n",
    "# h.weight_hh_l0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,model_name):\n",
    "    model_state_dict = model.state_dict()\n",
    "    model_state_dict.pop('word_embeddings.weight')\n",
    "    torch.save(model_state_dict,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(option,model_name):\n",
    "    torch.manual_seed(1)\n",
    "    model = LSTMTagger(embedding_dim, option, vocab_size, out_size)\n",
    "    model_state_dict = torch.load(model_name)\n",
    "    model_state_dict['word_embeddings.weight'] = torch.from_numpy(weights).float()\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    # Freeze\n",
    "    model.word_embeddings.weight.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensor(vec, pad, dim):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        vec - tensor to pad\n",
    "        pad - the size to pad to\n",
    "        dim - dimension to pad\n",
    "\n",
    "    return:\n",
    "        a new tensor padded to 'pad' in dimension 'dim'\n",
    "    \"\"\"\n",
    "    pad_size = list(vec.shape)\n",
    "    pad_size[dim] = pad - vec.size(dim)\n",
    "    res = torch.cat([vec, (torch.zeros(*pad_size,dtype=torch.long).cuda() \\\n",
    "                           if cuda else torch.zeros(*pad_size,dtype=torch.long))], dim=dim)\n",
    "    if cuda:\n",
    "        res = res.cuda()\n",
    "    return res\n",
    "\n",
    "class PadCollate:\n",
    "    \"\"\"\n",
    "    a variant of callate_fn that pads according to the longest sequence in\n",
    "    a batch of sequences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim=0):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            dim - the dimension to be padded (dimension of time in sequences)\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "\n",
    "    def pad_collate(self, batch):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            batch - list of (tensor, label)\n",
    "\n",
    "        reutrn:\n",
    "            xs - a tensor of all examples in 'batch' after padding\n",
    "            ys - a LongTensor of all labels in batch\n",
    "        \"\"\"\n",
    "        lengths = list(map(lambda x: x[0].shape[self.dim], batch))\n",
    "        # find longest sequence\n",
    "        max_len = max(lengths)\n",
    "        # pad according to max_len\n",
    "        xs = torch.zeros([len(lengths),max_len],dtype=torch.long)\n",
    "        if cuda:\n",
    "            xs = xs.cuda()\n",
    "        for idx,(seq,seqlen) in enumerate(zip(batch,lengths)):\n",
    "            xs[idx,:seqlen] = seq[0]\n",
    "        ys = torch.FloatTensor(list(map(lambda x: x[1], batch)))\n",
    "        lengths_tensor = torch.tensor(lengths)\n",
    "\n",
    "        if cuda:\n",
    "            xs = xs.cuda()\n",
    "            ys = ys.cuda()\n",
    "            lengths_tensor = lengths_tensor.cuda()\n",
    "        return xs, ys, lengths_tensor\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return self.pad_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vec(sentence,debug=False):\n",
    "    sentence = sentence.replace(\".\",\" \").replace(\",\",\" \").replace(\"!\",\" \").replace(\"'\",\" \").replace(\"\\\"\",\" \").replace(\"“\",\" \").replace(\"”\",\" \")\n",
    "    w_list = word_tokenize(sentence)\n",
    "    w_list = [wordnet.morphy(w).lower() if wordnet.morphy(w) is not None else w.lower() for w in w_list]\n",
    "    if debug:\n",
    "        print(w_list)\n",
    "    res_seq = prepare_sequence(w_list)\n",
    "    return res_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred(model,sentences):\n",
    "    # args\n",
    "    #   model : loaded model\n",
    "    #   sentences : list of sentence\n",
    "    sentence_vec = [(sentence2vec(sentence),0) for sentence in sentences]\n",
    "    pad = PadCollate(dim=0)\n",
    "    sentence_tensor,_,lengths = pad(sentence_vec)\n",
    "    lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "    sentence_tensor = sentence_tensor[perm_idx]\n",
    "\n",
    "    model.zero_grad()\n",
    "    model.hidden = model.init_hidden(len(sentences))        \n",
    "    y = model(sentence_tensor,lengths)\n",
    "    \n",
    "    _, inv_perm_idx = perm_idx.sort(0)\n",
    "\n",
    "    # print(perm_idx[inv_perm_idx])\n",
    "    # this tensor is [0,1,2,..]\n",
    "    y = y[inv_perm_idx]\n",
    "    return y.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred_va(models,sentences):\n",
    "    v = make_pred(models['Valence'],sentences)\n",
    "    a = make_pred(models['Arousal'],sentences)\n",
    "    res = np.r_['1',v,a]\n",
    "    # res : nparray of [v,a] for sentences\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred_va_sentence(models, sentence):\n",
    "    # arg \n",
    "    # sentence : string\n",
    "    v = float(make_pred(models['Valence'],[sentence]))\n",
    "    a = float(make_pred(models['Arousal'],[sentence]))\n",
    "    # res : float value of v,a\n",
    "    return v,a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = {\n",
    "    'hidden_size': 60,\n",
    "    'num_layers' : 1,\n",
    "    'bidirectional' : False,\n",
    "}\n",
    "vad_type = 'Arousal'\n",
    "bi = (1 if ops['bidirectional'] else 0)\n",
    "bs = 200\n",
    "lr = 0.001\n",
    "optimizer = 'Adagrad'\n",
    "model_name = './dat_model_json/best/{}_layer_{}_bi_{}_hd_{}_bs_{}_lr_{}_{}'.format(\n",
    "    vad_type,ops['num_layers'],bi,ops['hidden_size'],bs,lr,optimizer\n",
    ")\n",
    "model_a = load_model(ops,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = {\n",
    "    'hidden_size': 240,\n",
    "    'num_layers' : 2,\n",
    "    'bidirectional' : True,\n",
    "}\n",
    "vad_type = 'Valence'\n",
    "bi = (1 if ops['bidirectional'] else 0)\n",
    "bs = 50\n",
    "lr = 0.03\n",
    "optimizer = 'Adagrad'\n",
    "model_name = './dat_model_json/best/{}_layer_{}_bi_{}_hd_{}_bs_{}_lr_{}_{}'.format(\n",
    "    vad_type,ops['num_layers'],bi,ops['hidden_size'],bs,lr,optimizer\n",
    ")\n",
    "model_v = load_model(ops,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Valence' : model_v,\n",
    "    'Arousal' : model_a\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9261203408241272, 0.07154953479766846)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"I love you very much\",    \n",
    "    \"I'm very angry\",\n",
    "    'Are you kidding?',\n",
    "    \"fugaaaaaaaaaaa\",\n",
    "]\n",
    "make_pred_va(models,sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
