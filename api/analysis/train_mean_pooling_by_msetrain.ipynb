{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data_preprocessed_cut_2_Valence.csv`と`data_preprocessed_cut_2_Arousal.csv`をtrainする  \n",
    "\n",
    "引数でdictを受け取るver\n",
    "devで相関係数を出す\n",
    "\n",
    "cross-validationをやる  \n",
    "mseでtrainingする -> 基本的にだめ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.test.utils import datapath, get_tmpfile\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# # transform : glove -> tmp\n",
    "# glove2word2vec('./stanford_glove.txt', './stanford_w2v.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "gensim_model = gensim.models.KeyedVectors.load_word2vec_format('./stanford_w2v.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = gensim_model.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 2196016\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = weights.shape[1]\n",
    "print(embedding_dim,weights.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.append(weights,np.zeros((1,embedding_dim)),axis=0)\n",
    "# 末尾にunknown_wordを追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196017\n"
     ]
    }
   ],
   "source": [
    "vocab_size = weights.shape[0]\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "# cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # wordのindexを取得\n",
    "# print(gensim_model.wv.vocab['always'].index)\n",
    "# # 100番目のwordを取得\n",
    "# print(gensim_model.wv.index2word[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights[0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq):\n",
    "    vocab = gensim_model.wv.vocab\n",
    "    idxs = [vocab[w].index if w in vocab else vocab_size - 1 for w in seq]\n",
    "    res = torch.tensor(idxs, dtype=torch.long)\n",
    "    if cuda:\n",
    "        res = res.cuda()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vec(sentence):\n",
    "    w_list = sentence.split()\n",
    "    res_seq = prepare_sequence(w_list)\n",
    "    return res_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = \"I'm always fucking you.\"\n",
    "# sentence2vec(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, option, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = option['hidden_size']\n",
    "        self.num_layers = option['num_layers']\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.bi = (2 if option['bidirectional'] else 1)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        option['input_size'] = embedding_dim\n",
    "        option['batch_first'] = True\n",
    "\n",
    "        self.lstm = nn.LSTM(**option)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.out = nn.Linear(self.hidden_dim*self.bi, tagset_size)\n",
    "        self.hidden = self.init_hidden(1)\n",
    "\n",
    "    def init_hidden(self,batch_size):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        h = torch.zeros(self.num_layers*self.bi, batch_size, self.hidden_dim)\n",
    "        c = torch.zeros(self.num_layers*self.bi, batch_size, self.hidden_dim)\n",
    "        if cuda:\n",
    "            h = h.cuda()\n",
    "            c = c.cuda()\n",
    "        return (h,c)\n",
    "\n",
    "    def forward(self, sentence, lengths):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "#         print(embeds.size())\n",
    "#         batch_size = embeds.size()[0]\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embeds,lengths,batch_first=True)\n",
    "        lstm_output, self.hidden = self.lstm(packed, self.hidden)\n",
    "        unpacked,_ = nn.utils.rnn.pad_packed_sequence(lstm_output,batch_first=True)\n",
    "        # print(unpacked.size())\n",
    "        # :batch * max(len(lengths)) * hidden\n",
    "        \n",
    "        unpacked = torch.mean(unpacked,1)\n",
    "        # print(unpacked.size())\n",
    "        # :batch * hidden\n",
    "        output = self.out(unpacked)\n",
    "        output = F.tanh(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(option):\n",
    "    # 学習済みパラメータ\n",
    "    torch.manual_seed(1)\n",
    "    model = LSTMTagger(embedding_dim, option, vocab_size, out_size)\n",
    "    model.word_embeddings = nn.Embedding.from_pretrained(torch.from_numpy(weights).float())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ops = {\n",
    "#     'hidden_size': 3,\n",
    "#     'num_layers' : 1,\n",
    "#     'bidirectional' : False,\n",
    "# }\n",
    "# model = make_model(ops)\n",
    "# bs = 4\n",
    "# model.zero_grad()\n",
    "# model.hidden = model.init_hidden(bs)\n",
    "# model(torch.randint(0,100,(bs,10),dtype=torch.long),[10 for _ in range(bs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,model_name):\n",
    "    model_state_dict = model.state_dict()\n",
    "    model_state_dict.pop('word_embeddings.weight')\n",
    "    torch.save(model_state_dict,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(hidden_dim,model_name):\n",
    "    torch.manual_seed(1)\n",
    "    model = LSTMTagger(embedding_dim, hidden_dim, vocab_size, out_size)\n",
    "    model_state_dict = torch.load(model_name)\n",
    "    model_state_dict['word_embeddings.weight'] = torch.from_numpy(weights).float()\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    # Freeze\n",
    "    model.word_embeddings.weight.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vad_type='Valence'\n",
    "# data_cut = pd.read_csv('./data_preprocessed_{0}.csv'.format(vad_type),encoding='utf-16')\n",
    "# data_cut = data_cut[data_cut['words']>=2]\n",
    "# X_train = data_cut[data_cut['data_type']=='train']['reg'].as_matrix()\n",
    "# Y_train = data_cut[data_cut['data_type']=='train'][['{0}_reg'.format(vad_type)]].as_matrix()\n",
    "# samples = []\n",
    "# for sentence,target in zip(X_train,Y_train):\n",
    "#     sentence_vec = sentence2vec(sentence)\n",
    "#     y_hat = torch.tensor(target, dtype=torch.float)\n",
    "#     if cuda:\n",
    "#         y_hat = y_hat.cuda()\n",
    "#     samples.append((sentence_vec,y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensor(vec, pad, dim):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        vec - tensor to pad\n",
    "        pad - the size to pad to\n",
    "        dim - dimension to pad\n",
    "\n",
    "    return:\n",
    "        a new tensor padded to 'pad' in dimension 'dim'\n",
    "    \"\"\"\n",
    "    pad_size = list(vec.shape)\n",
    "    pad_size[dim] = pad - vec.size(dim)\n",
    "    res = torch.cat([vec, (torch.zeros(*pad_size,dtype=torch.long).cuda() \\\n",
    "                           if cuda else torch.zeros(*pad_size,dtype=torch.long))], dim=dim)\n",
    "    if cuda:\n",
    "        res = res.cuda()\n",
    "    return res\n",
    "\n",
    "class PadCollate:\n",
    "    \"\"\"\n",
    "    a variant of callate_fn that pads according to the longest sequence in\n",
    "    a batch of sequences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim=0):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            dim - the dimension to be padded (dimension of time in sequences)\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "\n",
    "    def pad_collate(self, batch):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            batch - list of (tensor, label)\n",
    "\n",
    "        reutrn:\n",
    "            xs - a tensor of all examples in 'batch' after padding\n",
    "            ys - a LongTensor of all labels in batch\n",
    "        \"\"\"\n",
    "        lengths = list(map(lambda x: x[0].shape[self.dim], batch))\n",
    "        # find longest sequence\n",
    "        max_len = max(lengths)\n",
    "        # pad according to max_len\n",
    "        xs = torch.zeros([len(lengths),max_len],dtype=torch.long)\n",
    "        if cuda:\n",
    "            xs = xs.cuda()\n",
    "        for idx,(seq,seqlen) in enumerate(zip(batch,lengths)):\n",
    "            xs[idx,:seqlen] = seq[0]\n",
    "        ys = torch.FloatTensor(list(map(lambda x: x[1], batch)))\n",
    "        lengths_tensor = torch.tensor(lengths)\n",
    "\n",
    "        if cuda:\n",
    "            xs = xs.cuda()\n",
    "            ys = ys.cuda()\n",
    "            lengths_tensor = lengths_tensor.cuda()\n",
    "        return xs, ys, lengths_tensor\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return self.pad_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(samples,batch_size=4,shuffle=True,collate_fn=PadCollate(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sam in train_loader:\n",
    "#     sam\n",
    "#     print(sam[0].size(),sam[1].size())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(X,Y):\n",
    "    ds = []\n",
    "    for sentence,target in zip(X,Y):\n",
    "        sentence_vec = sentence2vec(sentence)\n",
    "        y_hat = torch.tensor(target, dtype=torch.float)\n",
    "        if cuda:\n",
    "            y_hat = y_hat.cuda()\n",
    "        ds.append((sentence_vec,y_hat))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_and_train_cross_validation_option(option,epochs,vad_type,csv_path='./data_cut_only.csv',metric='MSELoss',\n",
    "                         save_dir='./dat_model_json',learning_rate=0.01,batch_size=2,\n",
    "                         optimizer_name='Adam',print_result=True):\n",
    "    \n",
    "    # args\n",
    "    # option : lstmに渡すoption\n",
    "    # csv_path : csvへのパス\n",
    "    # save_dir : modelとjsonをsaveするdir (中にjson,modelディレクトリを含む)\n",
    "    \n",
    "    if not 'hidden_size' in option:\n",
    "        print('hidden_size is necessary')\n",
    "        return\n",
    "    hidden_dim = option['hidden_size']\n",
    "    \n",
    "    if not 'bidirectional' in option:\n",
    "        bidirectional = 0\n",
    "    else:\n",
    "        bidirectional = (1 if option['bidirectional'] else 0)\n",
    "    \n",
    "    if not 'num_layers' in option:\n",
    "        num_layers = 1\n",
    "    else:\n",
    "        num_layers = option['num_layers']\n",
    "        \n",
    "    if not 'dropout' in option:\n",
    "        dropout = 0\n",
    "    else:\n",
    "        dropout = option['dropout']\n",
    "    \n",
    "    json_name = './{0}./json/{1}_layer_{2}_bi_{3}_hd_{4}_bs_{5}_lr_{6}_dr_{7}_{8}.json'.format(\\\n",
    "                        save_dir,vad_type,num_layers,bidirectional,hidden_dim,batch_size,learning_rate,dropout,optimizer_name)\n",
    "    # modelはsaveしない\n",
    "#     base_model_name = './{0}/model/{1}_layer_{2}_bi_{3}_hd_{4}_bs_{5}_lr_{6}'.format(\\\n",
    "#                         save_dir,vad_type,num_layers,bidirectional,hidden_dim,batch_size,learning_rate)\n",
    "\n",
    "\n",
    "    epoch_start = 0\n",
    "\n",
    "\n",
    "    data_cut = pd.read_csv('{}'.format(csv_path),encoding='utf-16')\n",
    "    data_cut = data_cut[data_cut['words']>=2]\n",
    "\n",
    "    X = data_cut['reg'].as_matrix()\n",
    "    Y = data_cut['{0}_reg'.format(vad_type)].as_matrix()\n",
    "\n",
    "\n",
    "    loss_function = nn.MSELoss(size_average=False)\n",
    "    loss_function_metric = nn.L1Loss(size_average=False)\n",
    "    \n",
    "#     loss_function_metric = nn.L1Loss(size_average=False)\n",
    "    \n",
    "\n",
    "    # import time\n",
    "    # t1 = time.time()\n",
    "\n",
    "    train_losses = []\n",
    "    dev_losses = []\n",
    "    dev_coefs = []\n",
    "\n",
    "    n_splits = 5\n",
    "    kf = KFold(n_splits=n_splits,shuffle=True)\n",
    "    # 5-fold\n",
    "    k_split = 1\n",
    "    \n",
    "    for train_index,dev_index in kf.split(X):\n",
    "        print(\"{} split / {}\".format(k_split,n_splits),flush=True)\n",
    "        \n",
    "        model = make_model(option)\n",
    "        \n",
    "        if cuda:\n",
    "            model.cuda()\n",
    "\n",
    "        if optimizer_name == 'Adadelta':\n",
    "            optimizer = optim.Adadelta(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "        elif optimizer_name == 'Adam':\n",
    "            optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "        elif optimizer_name == 'Adagrad':\n",
    "            optimizer = optim.Adagrad(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "        else:\n",
    "            print('{} couldnt be found'.format(optimizer_name))\n",
    "        \n",
    "        \n",
    "        k_split += 1\n",
    "        train_loss_part = []\n",
    "        dev_loss_part = []\n",
    "        dev_coef_part = []\n",
    "        \n",
    "        X_train, X_dev = X[train_index], X[dev_index]\n",
    "        Y_train, Y_dev = Y[train_index], Y[dev_index]\n",
    "        \n",
    "        ds_train = make_dataset(X_train,Y_train)\n",
    "        train_loader = DataLoader(ds_train,batch_size=batch_size,shuffle=True,collate_fn=PadCollate(dim=0))\n",
    "\n",
    "        ds_dev = make_dataset(X_dev,Y_dev)\n",
    "        dev_loader = DataLoader(ds_dev,batch_size=batch_size,shuffle=True,collate_fn=PadCollate(dim=0))\n",
    "\n",
    "        for epoch in range(epoch_start+1,epoch_start+1+epochs):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "            train_loss_sum = 0\n",
    "            \n",
    "            cnt = 0\n",
    "            for x_batch,y_batch,lengths in train_loader:\n",
    "                model.zero_grad()\n",
    "                model.hidden = model.init_hidden(len(x_batch))\n",
    "\n",
    "                # sort\n",
    "                lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "                x_batch = x_batch[perm_idx]\n",
    "                y_batch = y_batch[perm_idx]\n",
    "\n",
    "                y = model(x_batch,lengths)\n",
    "                y = y.view(-1)\n",
    "                loss = loss_function(y, y_batch)\n",
    "                \n",
    "                # training Loss func\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                loss_metric = loss_function_metric(y, y_batch)\n",
    "                train_loss_sum += loss_metric.data.item()\n",
    "                \n",
    "                \n",
    "                \n",
    "            train_loss_av = train_loss_sum/len(X_train)\n",
    "            train_loss_part.append(train_loss_av)            \n",
    "            \n",
    "            print(\"train : {} / {} = {}\".format(train_loss_sum,len(X_train),train_loss_av))\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            dev_loss_sum = 0\n",
    "\n",
    "            for x_batch,y_batch,lengths in dev_loader:\n",
    "                model.zero_grad()\n",
    "                model.hidden = model.init_hidden(len(x_batch))\n",
    "\n",
    "                # sort\n",
    "                lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "                x_batch = x_batch[perm_idx]\n",
    "                y_batch = y_batch[perm_idx]\n",
    "                y_true.append(y_batch.cpu().detach().numpy()[0])\n",
    "\n",
    "                y = model(x_batch,lengths)\n",
    "                y = y.view(-1)\n",
    "                y_pred.append(y.cpu().detach().numpy()[0])\n",
    "                \n",
    "                loss_metric = loss_function_metric(y, y_batch)\n",
    "                dev_loss_sum += loss_metric.data.item()\n",
    "\n",
    "            dev_loss_av = dev_loss_sum / len(X_dev)\n",
    "            dev_loss_part.append(dev_loss_av)\n",
    "            r = np.corrcoef(y_true,y_pred)[0,1]\n",
    "            dev_coef_part.append(r)\n",
    "                \n",
    "            print(\"dev : {} / {} = {}\".format(dev_loss_sum,len(X_dev),dev_loss_av))\n",
    "\n",
    "            # saveしない\n",
    "#             if epoch%10==0:\n",
    "#                 save_model(model,base_model_name+\"_epoch_{0}\".format(epoch))\n",
    "\n",
    "            if print_result:\n",
    "                print(\"epoch {0}: loss {1}\".format(epoch,train_loss_av),flush=True)\n",
    "            \n",
    "        train_losses.append(train_loss_part)\n",
    "        dev_losses.append(dev_loss_part)\n",
    "        dev_coefs.append(dev_coef_part)\n",
    "        \n",
    "        \n",
    "#     print(train_losses,dev_losses,dev_coefs)\n",
    "    train_loss = np.average(np.array(train_losses),axis=0).tolist()\n",
    "    dev_loss = np.average(np.array(dev_losses),axis=0).tolist()\n",
    "    dev_coef = np.average(np.array(dev_coefs),axis=0).tolist()\n",
    "    \n",
    "#     print(train_loss,dev_loss,dev_coef)\n",
    "#     print(train_losses)\n",
    "#     print(train_loss)\n",
    "#     print(dev_losses)\n",
    "#     print(dev_loss)\n",
    "    # t2 = time.time()\n",
    "\n",
    "    loss_data = {\n",
    "        'train' : train_loss,\n",
    "        'dev' : dev_loss,\n",
    "        'coef' : dev_coef\n",
    "    }\n",
    "    with open(json_name,'w') as f:\n",
    "        json.dump(loss_data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 split / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 687.6169626712799 / 4473 = 0.15372612623994633\n",
      "dev : 177.4210500717163 / 1119 = 0.1585532172222666\n",
      "epoch 1: loss 0.15372612623994633\n",
      "2 split / 5\n",
      "train : 687.0601637363434 / 4473 = 0.1536016462634347\n",
      "dev : 178.13132238388062 / 1119 = 0.1591879556603044\n",
      "epoch 1: loss 0.1536016462634347\n",
      "3 split / 5\n",
      "train : 690.4245982170105 / 4474 = 0.1543193111794838\n",
      "dev : 175.33347988128662 / 1118 = 0.15682779953603454\n",
      "epoch 1: loss 0.1543193111794838\n",
      "4 split / 5\n",
      "train : 697.8667044639587 / 4474 = 0.1559827233938218\n",
      "dev : 166.73427653312683 / 1118 = 0.14913620441245692\n",
      "epoch 1: loss 0.1559827233938218\n",
      "5 split / 5\n",
      "train : 697.4679980278015 / 4474 = 0.15589360706924485\n",
      "dev : 167.69691824913025 / 1118 = 0.14999724351442778\n",
      "epoch 1: loss 0.15589360706924485\n"
     ]
    }
   ],
   "source": [
    "# op1 = {\n",
    "#     'hidden_size' : 60,\n",
    "#     'bidirectional' : False,   \n",
    "#     'num_layers' : 2,\n",
    "#     'dropout' : 0.5\n",
    "# }\n",
    "# # op2 = {\n",
    "# #     'hidden_size' : 32,\n",
    "# #     'bidirectional' : True,   \n",
    "# #     'num_layers' : 3,\n",
    "# # }\n",
    "# make_model_and_train_cross_validation_option(op1,1,'Valence',metric='L1Loss',csv_path='./data_cut_only.csv',\n",
    "#                             save_dir='./dat_model_json/dat_cross_valid_word_cut_only/',\n",
    "#                             learning_rate=2e-5,batch_size=50,\n",
    "#                             optimizer_name='Adagrad')\n",
    "# # # make_model_and_train_option(op2,10,'Valence',metric='L1Loss',dat_base_name='./data_preprocessed_cut_2',\n",
    "# # #                             save_dir='./dat_model_json/dat_word_cut_l1loss_mullayer_bidirectional',\n",
    "# # #                             learning_rate=0.01,batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adadeltaとdropout入れた\n",
    "# vad_types = ['Valence','Arousal']\n",
    "vad_types = ['Valence']\n",
    "# bss = [4]\n",
    "bss = [50]\n",
    "# Adadeltaは特にlearning rateを探索しなくて良い\n",
    "# lrs = [1e-3,1e-4,5e-5]\n",
    "# lrs = [0.05]\n",
    "lrs = [0.5]\n",
    "# for Adadelta\n",
    "options = []\n",
    "\n",
    "hidden_dims = [600]\n",
    "num_layers = [2]\n",
    "bis = [True]\n",
    "drs = [0.25,0.5]\n",
    "\n",
    "for num_layer in num_layers:\n",
    "    for bi in bis:\n",
    "        for hidden_dim in hidden_dims:\n",
    "            for dr in drs:\n",
    "                options.append({\n",
    "                    'hidden_size' : hidden_dim,\n",
    "                    'bidirectional' : bi,   \n",
    "                    'num_layers' : num_layer,\n",
    "                    'dropout' : dr\n",
    "                })\n",
    "\n",
    "cnt = 0\n",
    "ma = len(vad_types)*len(lrs)*len(bss)*len(options)*len(drs)\n",
    "\n",
    "for vad_type in vad_types:\n",
    "    for lr in lrs:\n",
    "        for bs in bss:\n",
    "            for option in options:\n",
    "                epoch_num = 50\n",
    "#                 make_model_and_train_cross_validation_option(option,epoch_num,vad_type,metric='L1Loss',csv_path='./data_cut_only.csv',\n",
    "#                             save_dir='./dat_model_json/dat_stanford/',\n",
    "#                             learning_rate=lr,batch_size=bs,print_result=True,optimizer='Adadelta')\n",
    "                make_model_and_train_cross_validation_option(option,epoch_num,vad_type,metric='L1Loss',csv_path='./data_cut_only.csv',\n",
    "                            save_dir='./dat_model_json/dat_mean_by_mse/',\n",
    "                            learning_rate=lr,batch_size=bs,print_result=False,optimizer_name='Adadelta')\n",
    "                cnt += 1\n",
    "                print('{}/{}'.format(cnt,ma),flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adadeltaとdropout入れた\n",
    "# vad_types = ['Valence','Arousal']\n",
    "vad_types = ['Valence']\n",
    "# bss = [4]\n",
    "bss = [50]\n",
    "# Adadeltaは特にlearning rateを探索しなくて良い\n",
    "# lrs = [1e-3,1e-4,5e-5]\n",
    "lrs = [0.05]\n",
    "# lrs = [0.5]\n",
    "# for Adadelta\n",
    "options = []\n",
    "\n",
    "hidden_dims = [600]\n",
    "num_layers = [2]\n",
    "bis = [True]\n",
    "drs = [0.25,0.5]\n",
    "\n",
    "for num_layer in num_layers:\n",
    "    for bi in bis:\n",
    "        for hidden_dim in hidden_dims:\n",
    "            for dr in drs:\n",
    "                options.append({\n",
    "                    'hidden_size' : hidden_dim,\n",
    "                    'bidirectional' : bi,   \n",
    "                    'num_layers' : num_layer,\n",
    "                    'dropout' : dr\n",
    "                })\n",
    "\n",
    "cnt = 0\n",
    "ma = len(vad_types)*len(lrs)*len(bss)*len(options)*len(drs)\n",
    "\n",
    "for vad_type in vad_types:\n",
    "    for lr in lrs:\n",
    "        for bs in bss:\n",
    "            for option in options:\n",
    "                epoch_num = 50\n",
    "#                 make_model_and_train_cross_validation_option(option,epoch_num,vad_type,metric='L1Loss',csv_path='./data_cut_only.csv',\n",
    "#                             save_dir='./dat_model_json/dat_stanford/',\n",
    "#                             learning_rate=lr,batch_size=bs,print_result=True,optimizer='Adadelta')\n",
    "                make_model_and_train_cross_validation_option(option,epoch_num,vad_type,metric='L1Loss',csv_path='./data_cut_only.csv',\n",
    "                            save_dir='./dat_model_json/dat_mean_by_mse/',\n",
    "                            learning_rate=lr,batch_size=bs,print_result=False,optimizer_name='Adagrad')\n",
    "                cnt += 1\n",
    "                print('{}/{}'.format(cnt,ma),flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
