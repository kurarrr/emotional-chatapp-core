{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data_preprocessed_cut_2_Valence.csv`と`data_preprocessed_cut_2_Arousal.csv`をtrainする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "gensim_model = gensim.models.KeyedVectors.load_word2vec_format('./word2vec/data.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = gensim_model.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 3000000\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = weights.shape[1]\n",
    "print(embedding_dim,weights.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.append(weights,np.zeros((1,embedding_dim)),axis=0)\n",
    "# 末尾にunknown_wordを追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000001\n"
     ]
    }
   ],
   "source": [
    "vocab_size = weights.shape[0]\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wordのindexを取得\n",
    "# print(gensim_model.wv.vocab['always'].index)\n",
    "# 100番目のwordを取得\n",
    "# print(gensim_model.wv.index2word[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq):\n",
    "    vocab = gensim_model.wv.vocab\n",
    "    idxs = [vocab[w].index if w in vocab else vocab_size - 1 for w in seq]\n",
    "    res = torch.tensor(idxs, dtype=torch.long)\n",
    "    if cuda:\n",
    "        res = res.cuda()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vec(sentence):\n",
    "    w_list = sentence.split()\n",
    "    res_seq = prepare_sequence(w_list)\n",
    "    return res_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = \"I'm always fucking you.\"\n",
    "# sentence2vec(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.out = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden(1)\n",
    "\n",
    "    def init_hidden(self,batch_size):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        h = torch.zeros(1, batch_size, self.hidden_dim)\n",
    "        c = torch.zeros(1, batch_size, self.hidden_dim)\n",
    "        if cuda:\n",
    "            h = h.cuda()\n",
    "            c = c.cuda()\n",
    "        return (h,c)\n",
    "\n",
    "    def forward(self, sentence, lengths):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "#         print(embeds.size())\n",
    "#         batch_size = embeds.size()[0]\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embeds,lengths,batch_first=True)\n",
    "        lstm_output, self.hidden = self.lstm(packed, self.hidden)\n",
    "        unpacked,_ = nn.utils.rnn.pad_packed_sequence(lstm_output,batch_first=True)\n",
    "        # batch * hidden \n",
    "        output = self.out(unpacked[:,-1,:])\n",
    "        output = F.tanh(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(hidden_dim):\n",
    "    # 学習済みパラメータ\n",
    "    torch.manual_seed(1)\n",
    "    model = LSTMTagger(embedding_dim, hidden_dim, vocab_size, out_size)\n",
    "    model.word_embeddings = nn.Embedding.from_pretrained(torch.from_numpy(weights).float())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,model_name):\n",
    "    model_state_dict = model.state_dict()\n",
    "    model_state_dict.pop('word_embeddings.weight')\n",
    "    torch.save(model_state_dict,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(hidden_dim,model_name):\n",
    "    torch.manual_seed(1)\n",
    "    model = LSTMTagger(embedding_dim, hidden_dim, vocab_size, out_size)\n",
    "    model_state_dict = torch.load(model_name)\n",
    "    model_state_dict['word_embeddings.weight'] = torch.from_numpy(weights).float()\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    # Freeze\n",
    "    model.word_embeddings.weight.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vad_type='Valence'\n",
    "# data_cut = pd.read_csv('./data_preprocessed_{0}.csv'.format(vad_type),encoding='utf-16')\n",
    "# data_cut = data_cut[data_cut['words']>=2]\n",
    "# X_train = data_cut[data_cut['data_type']=='train']['reg'].as_matrix()\n",
    "# Y_train = data_cut[data_cut['data_type']=='train'][['{0}_reg'.format(vad_type)]].as_matrix()\n",
    "# samples = []\n",
    "# for sentence,target in zip(X_train,Y_train):\n",
    "#     sentence_vec = sentence2vec(sentence)\n",
    "#     y_hat = torch.tensor(target, dtype=torch.float)\n",
    "#     if cuda:\n",
    "#         y_hat = y_hat.cuda()\n",
    "#     samples.append((sentence_vec,y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensor(vec, pad, dim):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        vec - tensor to pad\n",
    "        pad - the size to pad to\n",
    "        dim - dimension to pad\n",
    "\n",
    "    return:\n",
    "        a new tensor padded to 'pad' in dimension 'dim'\n",
    "    \"\"\"\n",
    "    pad_size = list(vec.shape)\n",
    "    pad_size[dim] = pad - vec.size(dim)\n",
    "    res = torch.cat([vec, (torch.zeros(*pad_size,dtype=torch.long).cuda() \\\n",
    "                           if cuda else torch.zeros(*pad_size,dtype=torch.long))], dim=dim)\n",
    "    if cuda:\n",
    "        res = res.cuda()\n",
    "    return res\n",
    "\n",
    "class PadCollate:\n",
    "    \"\"\"\n",
    "    a variant of callate_fn that pads according to the longest sequence in\n",
    "    a batch of sequences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim=0):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            dim - the dimension to be padded (dimension of time in sequences)\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "\n",
    "    def pad_collate(self, batch):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            batch - list of (tensor, label)\n",
    "\n",
    "        reutrn:\n",
    "            xs - a tensor of all examples in 'batch' after padding\n",
    "            ys - a LongTensor of all labels in batch\n",
    "        \"\"\"\n",
    "        lengths = list(map(lambda x: x[0].shape[self.dim], batch))\n",
    "        # find longest sequence\n",
    "        max_len = max(lengths)\n",
    "        # pad according to max_len\n",
    "        xs = torch.zeros([len(lengths),max_len],dtype=torch.long)\n",
    "        if cuda:\n",
    "            xs = xs.cuda()\n",
    "        for idx,(seq,seqlen) in enumerate(zip(batch,lengths)):\n",
    "            xs[idx,:seqlen] = seq[0]\n",
    "        ys = torch.FloatTensor(list(map(lambda x: x[1], batch)))\n",
    "        lengths_tensor = torch.tensor(lengths)\n",
    "\n",
    "        if cuda:\n",
    "            xs = xs.cuda()\n",
    "            ys = ys.cuda()\n",
    "            lengths_tensor = lengths_tensor.cuda()\n",
    "        return xs, ys, lengths_tensor\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return self.pad_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(samples,batch_size=4,shuffle=True,collate_fn=PadCollate(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sam in train_loader:\n",
    "#     sam\n",
    "#     print(sam[0].size(),sam[1].size())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(X,Y):\n",
    "    ds = []\n",
    "    for sentence,target in zip(X,Y):\n",
    "        sentence_vec = sentence2vec(sentence)\n",
    "        y_hat = torch.tensor(target, dtype=torch.float)\n",
    "        if cuda:\n",
    "            y_hat = y_hat.cuda()\n",
    "        ds.append((sentence_vec,y_hat))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_and_train(hidden_dim,epochs,vad_type,dat_base_name='./data_preprocessed',metric='MSELoss',\n",
    "                         save_dir='./dat_model_json',learning_rate=0.01,batch_size=2,overwrite=False,model_name=\"\"):\n",
    "    # args\n",
    "    # dat_base_name : _{Valence,Arousal}.csv以前へのパス\n",
    "    # save_dor : modelとjsonをsaveするdir (中にjson,modelディレクトリを含む)\n",
    "    json_name = '{0}/json/loss_{1}_hidden_dim_{2}_batch_{3}_lr_{4}.json'.format(\\\n",
    "                        save_dir,vad_type,hidden_dim,batch_size,learning_rate)\n",
    "    base_model_name = '{0}/model/model_{1}_hidden_dim_{2}_batch_{3}_lr_{4}'.format(\\\n",
    "                        save_dir,vad_type,hidden_dim,batch_size,learning_rate)\n",
    "\n",
    "    if not overwrite:\n",
    "        model = make_model(hidden_dim)\n",
    "        train_loss = []\n",
    "        dev_loss = []\n",
    "        epoch_start = 0\n",
    "    else:\n",
    "        model = load_model(hidden_dim,model_name)\n",
    "        with open(json_name,'r') as f:\n",
    "            dat = json.load(f)            \n",
    "        train_loss = dat['train']\n",
    "        dev_loss = dat['dev']\n",
    "        epoch_start = len(train_loss)\n",
    "\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    data_cut = pd.read_csv('{0}_{1}.csv'.format(dat_base_name,vad_type),encoding='utf-16')\n",
    "    data_cut = data_cut[data_cut['words']>=2]\n",
    "\n",
    "    X_train = data_cut[data_cut['data_type']=='train']['reg'].as_matrix()\n",
    "    X_dev   = data_cut[data_cut['data_type']=='dev']['reg'].as_matrix()\n",
    "    X_test  = data_cut[data_cut['data_type']=='test']['reg'].as_matrix()\n",
    "    Y_train = data_cut[data_cut['data_type']=='train'][['{0}_reg'.format(vad_type)]].as_matrix()\n",
    "    Y_dev   = data_cut[data_cut['data_type']=='dev'][['{0}_reg'.format(vad_type)]].as_matrix()\n",
    "    Y_test  = data_cut[data_cut['data_type']=='test'][['{0}_reg'.format(vad_type)]].as_matrix()\n",
    "\n",
    "    ds_train = make_dataset(X_train,Y_train)\n",
    "    train_loader = DataLoader(ds_train,batch_size=batch_size,shuffle=True,collate_fn=PadCollate(dim=0))\n",
    "\n",
    "    ds_dev = make_dataset(X_dev,Y_dev)\n",
    "    dev_loader = DataLoader(ds_dev,batch_size=batch_size,shuffle=True,collate_fn=PadCollate(dim=0))\n",
    "\n",
    "    if metric == 'MSELoss':\n",
    "        loss_function = nn.MSELoss(size_average=False)\n",
    "    elif metric == 'L1Loss':\n",
    "        loss_function = nn.L1Loss(size_average=False)\n",
    "    else:\n",
    "        print('no loss funciton')\n",
    "        return\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "\n",
    "    # import time\n",
    "    # t1 = time.time()\n",
    "\n",
    "    for epoch in range(epoch_start+1,epoch_start+1+epochs):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "        train_loss_sum = 0\n",
    "        \n",
    "        for x_batch,y_batch,lengths in train_loader:\n",
    "            model.zero_grad()\n",
    "            model.hidden = model.init_hidden(len(x_batch))\n",
    "            \n",
    "            # sort\n",
    "            lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "            x_batch = x_batch[perm_idx]\n",
    "            y_batch = y_batch[perm_idx]\n",
    "\n",
    "            y = model(x_batch,lengths)\n",
    "            y = y.view(-1)\n",
    "            loss = loss_function(y, y_batch)\n",
    "            train_loss_sum += loss.data.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch%5==0:\n",
    "            dev_loss_sum = 0\n",
    "            for x_batch,y_batch,lengths in dev_loader:\n",
    "                model.zero_grad()\n",
    "                model.hidden = model.init_hidden(len(x_batch))\n",
    "                \n",
    "                # sort\n",
    "                lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "                x_batch = x_batch[perm_idx]\n",
    "                y_batch = y_batch[perm_idx]\n",
    "\n",
    "                y = model(x_batch,lengths)\n",
    "                y = y.view(-1)\n",
    "                loss = loss_function(y, y_batch)\n",
    "                dev_loss_sum += loss.data.item()\n",
    "\n",
    "            dev_loss_av = dev_loss_sum / len(X_dev)\n",
    "            dev_loss.append(dev_loss_av)\n",
    "          \n",
    "        if epoch%10==0:\n",
    "            save_model(model,base_model_name+\"_epoch_{0}\".format(epoch))\n",
    "        \n",
    "        train_loss_av = train_loss_sum/len(X_train)\n",
    "        print(\"epoch {0}: loss {1}\".format(epoch,train_loss_av))\n",
    "        train_loss.append(train_loss_av)\n",
    "\n",
    "    # t2 = time.time()\n",
    "    ds_test = make_dataset(X_test,Y_test)\n",
    "    test_loader = DataLoader(ds_test,batch_size=batch_size,shuffle=False,collate_fn=PadCollate(dim=0))\n",
    "    test_loss_sum = 0\n",
    "    for x_batch,y_batch,lengths in test_loader:\n",
    "        model.zero_grad()\n",
    "        model.hidden = model.init_hidden(len(x_batch))\n",
    "        # sort\n",
    "        lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "        x_batch = x_batch[perm_idx]\n",
    "        y_batch = y_batch[perm_idx]\n",
    "        y = model(x_batch,lengths)\n",
    "        y = y.view(-1)\n",
    "        loss = loss_function(y, y_batch)\n",
    "        test_loss_sum += loss.data.item()\n",
    "    test_loss = test_loss_sum / len(X_test)\n",
    "    # print(t2-t1)\n",
    "\n",
    "    loss_data = {\n",
    "        'train' : train_loss,\n",
    "        'dev' : dev_loss,\n",
    "        'test' : test_loss\n",
    "    }\n",
    "    with open(json_name,'w') as f:\n",
    "        json.dump(loss_data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'loss_function' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-e0adc2525a62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m make_model_and_train(8,9,'Valence',metric='L1Loss',dat_base_name='./data_preprocessed_cut_2',\n\u001b[0;32m----> 2\u001b[0;31m                          save_dir='./dat_model_json/dat_word_cut',learning_rate=0.01,batch_size=4,overwrite=False,model_name=\"\")\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-042e79481118>\u001b[0m in \u001b[0;36mmake_model_and_train\u001b[0;34m(hidden_dim, epochs, vad_type, dat_base_name, metric, save_dir, learning_rate, batch_size, overwrite, model_name)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mtrain_loss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'loss_function' referenced before assignment"
     ]
    }
   ],
   "source": [
    "make_model_and_train(8,9,'Valence',metric='L1Loss',dat_base_name='./data_preprocessed_cut_2',\n",
    "                         save_dir='./dat_model_json/dat_word_cut',learning_rate=0.01,batch_size=4,overwrite=False,model_name=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_types = ['Valence','Arousal']\n",
    "hidden_dims = [8,16,32,64,128]\n",
    "bss = [4,8,16,32]\n",
    "lrs = [0.01, 0.005,0.001]\n",
    "for lr in lrs:\n",
    "    for bs in bss:\n",
    "        for hidden_dim in hidden_dims:\n",
    "            for vad_type in vad_types:\n",
    "                epoch_num = 50\n",
    "                make_model_and_train(hidden_dim,epoch_num,vad_type,dat_base_name='./data_preprocessed_cut_2',\n",
    "                             save_dir='./dat_model_json/dat_word_cut',learning_rate=lr,batch_size=bs,overwrite=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_prediction(hidden_dim,vad_type,model_name,df):\n",
    "\n",
    "#     model = load_model(hidden_dim,model_name)\n",
    "\n",
    "#     if cuda:\n",
    "#         model.cuda()\n",
    "\n",
    "#     X = df['reg'].as_matrix()\n",
    "       \n",
    "#     pred_list = []\n",
    "\n",
    "#     for sentence in X:\n",
    "#         sentence_vec = sentence2vec(sentence)\n",
    "#         sentence_vec = sentence_vec.view(1,-1)\n",
    "#         print\n",
    "#         model.zero_grad()\n",
    "#         model.hidden = model.init_hidden(1)\n",
    "\n",
    "#         y = model(sentence_vec,[sentence_vec.size()[1]])\n",
    "#         y = y.view(-1)\n",
    "#         if cuda:\n",
    "#             y = y.cpu()\n",
    "#         pred_list.append(y.detach().numpy())\n",
    "    \n",
    "#     pred_list = np.array(pred_list)\n",
    "    \n",
    "#     return pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# hidden_dim = 16\n",
    "# model_path = {\n",
    "#     'Valence' : './dat_to1_3/model_Valence_hidden_dim_16_batch_2_lr_0.01_epoch_50',\n",
    "#     'Arousal' : './dat_to1_3/model_Arousal_hidden_dim_16_batch_2_lr_0.01_epoch_50',\n",
    "#     'Dominance' : './dat_to1_3/model_Dominance_hidden_dim_16_batch_2_lr_0.01_epoch_50'\n",
    "# }\n",
    "# # vad_types = ['Valence']\n",
    "# vad_types = ['Valence','Arousal','Dominance']\n",
    "\n",
    "# for vad_type in vad_types:\n",
    "#     df = pd.read_csv('./data_preprocessed_{0}.csv'.format(vad_type),encoding='utf-16')\n",
    "#     df = df[df['words']>=2]\n",
    "#     df = df.reset_index()\n",
    "#     ary = make_prediction(hidden_dim,vad_type,model_path[vad_type],df)\n",
    "#     ary = ary * 2 + 3\n",
    "#     pred_df = df.assign(\n",
    "#             **{ \"{0}_pred\".format(vad_type) : pd.Series(ary.reshape(-1))}\n",
    "#         )\n",
    "#     to_name = \"./prediction/{0}_2.csv\".format(vad_type)\n",
    "\n",
    "#     pred_df.to_csv(to_name,encoding='utf-16',sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
