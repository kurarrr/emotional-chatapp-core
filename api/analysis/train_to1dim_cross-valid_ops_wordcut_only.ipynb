{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data_preprocessed_cut_2_Valence.csv`と`data_preprocessed_cut_2_Arousal.csv`をtrainする  \n",
    "\n",
    "引数でdictを受け取るver\n",
    "devで相関係数を出す\n",
    "\n",
    "cross-validationをやる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "gensim_model = gensim.models.KeyedVectors.load_word2vec_format('./word2vec/data.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = gensim_model.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 3000000\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = weights.shape[1]\n",
    "print(embedding_dim,weights.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.append(weights,np.zeros((1,embedding_dim)),axis=0)\n",
    "# 末尾にunknown_wordを追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000001\n"
     ]
    }
   ],
   "source": [
    "vocab_size = weights.shape[0]\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "# cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wordのindexを取得\n",
    "# print(gensim_model.wv.vocab['always'].index)\n",
    "# 100番目のwordを取得\n",
    "# print(gensim_model.wv.index2word[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq):\n",
    "    vocab = gensim_model.wv.vocab\n",
    "    idxs = [vocab[w].index if w in vocab else vocab_size - 1 for w in seq]\n",
    "    res = torch.tensor(idxs, dtype=torch.long)\n",
    "    if cuda:\n",
    "        res = res.cuda()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vec(sentence):\n",
    "    w_list = sentence.split()\n",
    "    res_seq = prepare_sequence(w_list)\n",
    "    return res_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = \"I'm always fucking you.\"\n",
    "# sentence2vec(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, option, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = option['hidden_size']\n",
    "        self.num_layers = option['num_layers']\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.bi = (2 if option['bidirectional'] else 1)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        option['input_size'] = embedding_dim\n",
    "        option['batch_first'] = True\n",
    "\n",
    "        self.lstm = nn.LSTM(**option)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.out = nn.Linear(self.hidden_dim*self.bi, tagset_size)\n",
    "        self.hidden = self.init_hidden(1)\n",
    "\n",
    "    def init_hidden(self,batch_size):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        h = torch.zeros(self.num_layers*self.bi, batch_size, self.hidden_dim)\n",
    "        c = torch.zeros(self.num_layers*self.bi, batch_size, self.hidden_dim)\n",
    "        if cuda:\n",
    "            h = h.cuda()\n",
    "            c = c.cuda()\n",
    "        return (h,c)\n",
    "\n",
    "    def forward(self, sentence, lengths):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "#         print(embeds.size())\n",
    "#         batch_size = embeds.size()[0]\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embeds,lengths,batch_first=True)\n",
    "        lstm_output, self.hidden = self.lstm(packed, self.hidden)\n",
    "        unpacked,_ = nn.utils.rnn.pad_packed_sequence(lstm_output,batch_first=True)\n",
    "        # batch * hidden \n",
    "        output = self.out(unpacked[:,-1,:])\n",
    "        output = F.tanh(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(option):\n",
    "    # 学習済みパラメータ\n",
    "    torch.manual_seed(1)\n",
    "    model = LSTMTagger(embedding_dim, option, vocab_size, out_size)\n",
    "    model.word_embeddings = nn.Embedding.from_pretrained(torch.from_numpy(weights).float())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ops = {\n",
    "#     'hidden_size': 3,\n",
    "#     'num_layers' : 2,\n",
    "#     'bidirectional' : True,\n",
    "# }\n",
    "# make_model(ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,model_name):\n",
    "    model_state_dict = model.state_dict()\n",
    "    model_state_dict.pop('word_embeddings.weight')\n",
    "    torch.save(model_state_dict,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(hidden_dim,model_name):\n",
    "    torch.manual_seed(1)\n",
    "    model = LSTMTagger(embedding_dim, hidden_dim, vocab_size, out_size)\n",
    "    model_state_dict = torch.load(model_name)\n",
    "    model_state_dict['word_embeddings.weight'] = torch.from_numpy(weights).float()\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    # Freeze\n",
    "    model.word_embeddings.weight.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vad_type='Valence'\n",
    "# data_cut = pd.read_csv('./data_preprocessed_{0}.csv'.format(vad_type),encoding='utf-16')\n",
    "# data_cut = data_cut[data_cut['words']>=2]\n",
    "# X_train = data_cut[data_cut['data_type']=='train']['reg'].as_matrix()\n",
    "# Y_train = data_cut[data_cut['data_type']=='train'][['{0}_reg'.format(vad_type)]].as_matrix()\n",
    "# samples = []\n",
    "# for sentence,target in zip(X_train,Y_train):\n",
    "#     sentence_vec = sentence2vec(sentence)\n",
    "#     y_hat = torch.tensor(target, dtype=torch.float)\n",
    "#     if cuda:\n",
    "#         y_hat = y_hat.cuda()\n",
    "#     samples.append((sentence_vec,y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensor(vec, pad, dim):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        vec - tensor to pad\n",
    "        pad - the size to pad to\n",
    "        dim - dimension to pad\n",
    "\n",
    "    return:\n",
    "        a new tensor padded to 'pad' in dimension 'dim'\n",
    "    \"\"\"\n",
    "    pad_size = list(vec.shape)\n",
    "    pad_size[dim] = pad - vec.size(dim)\n",
    "    res = torch.cat([vec, (torch.zeros(*pad_size,dtype=torch.long).cuda() \\\n",
    "                           if cuda else torch.zeros(*pad_size,dtype=torch.long))], dim=dim)\n",
    "    if cuda:\n",
    "        res = res.cuda()\n",
    "    return res\n",
    "\n",
    "class PadCollate:\n",
    "    \"\"\"\n",
    "    a variant of callate_fn that pads according to the longest sequence in\n",
    "    a batch of sequences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim=0):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            dim - the dimension to be padded (dimension of time in sequences)\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "\n",
    "    def pad_collate(self, batch):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            batch - list of (tensor, label)\n",
    "\n",
    "        reutrn:\n",
    "            xs - a tensor of all examples in 'batch' after padding\n",
    "            ys - a LongTensor of all labels in batch\n",
    "        \"\"\"\n",
    "        lengths = list(map(lambda x: x[0].shape[self.dim], batch))\n",
    "        # find longest sequence\n",
    "        max_len = max(lengths)\n",
    "        # pad according to max_len\n",
    "        xs = torch.zeros([len(lengths),max_len],dtype=torch.long)\n",
    "        if cuda:\n",
    "            xs = xs.cuda()\n",
    "        for idx,(seq,seqlen) in enumerate(zip(batch,lengths)):\n",
    "            xs[idx,:seqlen] = seq[0]\n",
    "        ys = torch.FloatTensor(list(map(lambda x: x[1], batch)))\n",
    "        lengths_tensor = torch.tensor(lengths)\n",
    "\n",
    "        if cuda:\n",
    "            xs = xs.cuda()\n",
    "            ys = ys.cuda()\n",
    "            lengths_tensor = lengths_tensor.cuda()\n",
    "        return xs, ys, lengths_tensor\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return self.pad_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(samples,batch_size=4,shuffle=True,collate_fn=PadCollate(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sam in train_loader:\n",
    "#     sam\n",
    "#     print(sam[0].size(),sam[1].size())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(X,Y):\n",
    "    ds = []\n",
    "    for sentence,target in zip(X,Y):\n",
    "        sentence_vec = sentence2vec(sentence)\n",
    "        y_hat = torch.tensor(target, dtype=torch.float)\n",
    "        if cuda:\n",
    "            y_hat = y_hat.cuda()\n",
    "        ds.append((sentence_vec,y_hat))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_and_train_cross_validation_option(option,epochs,vad_type,csv_path='./data_cut_only.csv',metric='MSELoss',\n",
    "                         save_dir='./dat_model_json',learning_rate=0.01,batch_size=2,print_result=True):\n",
    "    \n",
    "    # args\n",
    "    # option : lstmに渡すoption\n",
    "    # csv_path : csvへのパス\n",
    "    # save_dir : modelとjsonをsaveするdir (中にjson,modelディレクトリを含む)\n",
    "    \n",
    "    if not 'hidden_size' in option:\n",
    "        print('hidden_size is necessary')\n",
    "        return\n",
    "    hidden_dim = option['hidden_size']\n",
    "    \n",
    "    if not 'bidirectional' in option:\n",
    "        bidirectional = 0\n",
    "    else:\n",
    "        bidirectional = (1 if option['bidirectional'] else 0)\n",
    "    \n",
    "    if not 'num_layers' in option:\n",
    "        num_layers = 1\n",
    "    else:\n",
    "        num_layers = option['num_layers']\n",
    "    \n",
    "    json_name = './{0}/json/{1}_layer_{2}_bi_{3}_hd_{4}_bs_{5}_lr_{6}.json'.format(\\\n",
    "                        save_dir,vad_type,num_layers,bidirectional,hidden_dim,batch_size,learning_rate)\n",
    "    # modelはsaveしない\n",
    "#     base_model_name = './{0}/model/{1}_layer_{2}_bi_{3}_hd_{4}_bs_{5}_lr_{6}'.format(\\\n",
    "#                         save_dir,vad_type,num_layers,bidirectional,hidden_dim,batch_size,learning_rate)\n",
    "\n",
    "    model = make_model(option)\n",
    "\n",
    "    epoch_start = 0\n",
    "\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    data_cut = pd.read_csv('{}'.format(csv_path),encoding='utf-16')\n",
    "    data_cut = data_cut[data_cut['words']>=2]\n",
    "\n",
    "    X = data_cut['reg'].as_matrix()\n",
    "    Y = data_cut['{0}_reg'.format(vad_type)].as_matrix()\n",
    "\n",
    "\n",
    "    if metric == 'MSELoss':\n",
    "        loss_function = nn.MSELoss(size_average=False)\n",
    "    elif metric == 'L1Loss':\n",
    "        loss_function = nn.L1Loss(size_average=False)\n",
    "    else:\n",
    "        print('no loss funciton')\n",
    "        return\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "\n",
    "    # import time\n",
    "    # t1 = time.time()\n",
    "\n",
    "    train_losses = []\n",
    "    dev_losses = []\n",
    "    dev_coefs = []\n",
    "\n",
    "    n_splits = 5\n",
    "    kf = KFold(n_splits=n_splits,shuffle=True)\n",
    "    # 5-fold\n",
    "\n",
    "    for train_index,dev_index in kf.split(X):\n",
    "        train_loss_part = []\n",
    "        dev_loss_part = []\n",
    "        dev_coef_part = []\n",
    "        \n",
    "        X_train, X_dev = X[train_index], X[dev_index]\n",
    "        Y_train, Y_dev = Y[train_index], Y[dev_index]\n",
    "        \n",
    "        ds_train = make_dataset(X_train,Y_train)\n",
    "        train_loader = DataLoader(ds_train,batch_size=batch_size,shuffle=True,collate_fn=PadCollate(dim=0))\n",
    "\n",
    "        ds_dev = make_dataset(X_dev,Y_dev)\n",
    "        dev_loader = DataLoader(ds_dev,batch_size=batch_size,shuffle=True,collate_fn=PadCollate(dim=0))\n",
    "\n",
    "        for epoch in range(epoch_start+1,epoch_start+1+epochs):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "            train_loss_sum = 0\n",
    "\n",
    "            for x_batch,y_batch,lengths in train_loader:\n",
    "                model.zero_grad()\n",
    "                model.hidden = model.init_hidden(len(x_batch))\n",
    "\n",
    "                # sort\n",
    "                lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "                x_batch = x_batch[perm_idx]\n",
    "                y_batch = y_batch[perm_idx]\n",
    "\n",
    "                y = model(x_batch,lengths)\n",
    "                y = y.view(-1)\n",
    "                loss = loss_function(y, y_batch)\n",
    "                train_loss_sum += loss.data.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            train_loss_av = train_loss_sum/len(X_train)\n",
    "            train_loss_part.append(train_loss_av)            \n",
    "\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            dev_loss_sum = 0\n",
    "\n",
    "            for x_batch,y_batch,lengths in dev_loader:\n",
    "                model.zero_grad()\n",
    "                model.hidden = model.init_hidden(len(x_batch))\n",
    "\n",
    "                # sort\n",
    "                lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "                x_batch = x_batch[perm_idx]\n",
    "                y_batch = y_batch[perm_idx]\n",
    "                y_true.append(y_batch.cpu().detach().numpy()[0])\n",
    "\n",
    "                y = model(x_batch,lengths)\n",
    "                y = y.view(-1)\n",
    "                y_pred.append(y.cpu().detach().numpy()[0])\n",
    "                loss = loss_function(y, y_batch)\n",
    "                dev_loss_sum += loss.data.item()\n",
    "\n",
    "            dev_loss_av = dev_loss_sum / len(X_dev)\n",
    "            dev_loss_part.append(dev_loss_av)\n",
    "            r = np.corrcoef(y_true,y_pred)[0,1]\n",
    "            dev_coef_part.append(r)\n",
    "                \n",
    "            # saveしない\n",
    "#             if epoch%10==0:\n",
    "#                 save_model(model,base_model_name+\"_epoch_{0}\".format(epoch))\n",
    "\n",
    "            if print_result:\n",
    "                print(\"epoch {0}: loss {1}\".format(epoch,train_loss_av))\n",
    "            \n",
    "        train_losses.append(train_loss_part)\n",
    "        dev_losses.append(dev_loss_part)\n",
    "        dev_coefs.append(dev_coef_part)\n",
    "        \n",
    "#     print(train_losses,dev_losses,dev_coefs)\n",
    "    train_loss = np.average(np.array(train_losses),axis=0).tolist()\n",
    "    dev_loss = np.average(np.array(dev_losses),axis=0).tolist()\n",
    "    dev_coef = np.average(np.array(dev_coefs),axis=0).tolist()\n",
    "    \n",
    "#     print(train_loss,dev_loss,dev_coef)\n",
    "\n",
    "    \n",
    "    # t2 = time.time()\n",
    "\n",
    "    loss_data = {\n",
    "        'train' : train_loss,\n",
    "        'dev' : dev_loss,\n",
    "        'coef' : dev_coef\n",
    "    }\n",
    "    with open(json_name,'w') as f:\n",
    "        json.dump(loss_data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# op1 = {\n",
    "#     'hidden_size' : 16,\n",
    "#     'bidirectional' : False,   \n",
    "#     'num_layers' : 2,\n",
    "# }\n",
    "# op2 = {\n",
    "#     'hidden_size' : 32,\n",
    "#     'bidirectional' : True,   \n",
    "#     'num_layers' : 3,\n",
    "# }\n",
    "# make_model_and_train_cross_validation_option(op1,4,'Valence',metric='L1Loss',csv_path='./data_cut_only.csv',\n",
    "#                             save_dir='./dat_model_json/dat_cross_valid_word_cut_only/',\n",
    "#                             learning_rate=2e-5,batch_size=4)\n",
    "# # make_model_and_train_option(op2,10,'Valence',metric='L1Loss',dat_base_name='./data_preprocessed_cut_2',\n",
    "# #                             save_dir='./dat_model_json/dat_word_cut_l1loss_mullayer_bidirectional',\n",
    "# #                             learning_rate=0.01,batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_types = ['Valence','Arousal']\n",
    "bss = [4,8]\n",
    "lrs = [1e-5,2e-5]\n",
    "\n",
    "options = []\n",
    "\n",
    "hidden_dims = [32,64]\n",
    "num_layers = [2]\n",
    "bis = [False]\n",
    "\n",
    "for num_layer in num_layers:\n",
    "    for bi in bis:\n",
    "        for hidden_dim in hidden_dims:\n",
    "            options.append({\n",
    "                'hidden_size' : hidden_dim,\n",
    "                'bidirectional' : bi,   \n",
    "                'num_layers' : num_layer,\n",
    "            })\n",
    "\n",
    "cnt = 0\n",
    "ma = len(vad_types)*len(lrs)*len(bss)*len(options)\n",
    "\n",
    "for vad_type in vad_types:\n",
    "    for lr in lrs:\n",
    "        for bs in bss:\n",
    "            for option in options:\n",
    "                epoch_num = 1\n",
    "                make_model_and_train_cross_validation_option(option,epoch_num,vad_type,metric='L1Loss',csv_path='./data_cut_only.csv',\n",
    "                            save_dir='./dat_model_json/dat_cross_valid_word_cut_only/',\n",
    "                            learning_rate=lr,batch_size=bs,print_result=False)\n",
    "\n",
    "                cnt += 1\n",
    "                print('{}/{}'.format(cnt,ma))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss 0.15555645204729052\n",
      "epoch 2: loss 0.15217569601255046\n",
      "epoch 3: loss 0.14911638222852996\n",
      "epoch 4: loss 0.1470260348853091\n",
      "epoch 5: loss 0.14457026833642117\n",
      "epoch 6: loss 0.14480754935840365\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-6402376e3229>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m                 make_model_and_train_option(option,epoch_num,vad_type,metric='L1Loss',csv_path='./data_cut_only.csv',\n\u001b[1;32m     26\u001b[0m                             \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./dat_model_json/dat_word_cut_only/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                             learning_rate=lr,batch_size=bs)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-b2e546e2efe4>\u001b[0m in \u001b[0;36mmake_model_and_train_option\u001b[0;34m(option, epochs, vad_type, csv_path, metric, save_dir, learning_rate, batch_size)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mtrain_loss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# vad_types = ['Valence','Arousal']\n",
    "# bss = [4,8]\n",
    "# lrs = [0.005,0.001,0.0001,0.00001]\n",
    "\n",
    "# options = []\n",
    "\n",
    "# hidden_dims = [32,64,128]\n",
    "# num_layers = [1,2]\n",
    "# bis = [True,False]\n",
    "\n",
    "# for num_layer in num_layers:\n",
    "#     for bi in bis:\n",
    "#         for hidden_dim in hidden_dims:\n",
    "#             options.append({\n",
    "#                 'hidden_size' : hidden_dim,\n",
    "#                 'bidirectional' : bi,   \n",
    "#                 'num_layers' : num_layer,\n",
    "#             })\n",
    "\n",
    "# cnt = 0\n",
    "# ma = len(vad_types)*len(lrs)*len(bss)*len(options)\n",
    "\n",
    "# for vad_type in vad_types:\n",
    "#     for lr in lrs:\n",
    "#         for bs in bss:\n",
    "#             for option in options:\n",
    "#                 epoch_num = 50\n",
    "#                 make_model_and_train_option(option,epoch_num,vad_type,metric='L1Loss',csv_path='./data_cut_only.csv',\n",
    "#                             save_dir='./dat_model_json/dat_word_cut_only/',\n",
    "#                             learning_rate=lr,batch_size=bs,print_result=False)\n",
    "#                 cnt += 1\n",
    "#                 print('{}/{}'.format(cnt,ma))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2    3   16 ... 5560 5585 5586]\n",
      "[   6    7   21 ... 5584 5587 5590]\n",
      "[   0   10   11 ... 5582 5588 5591]\n",
      "[   5    9   14 ... 5574 5576 5577]\n",
      "[   1    4    8 ... 5580 5581 5589]\n"
     ]
    }
   ],
   "source": [
    "# K-Fold の test\n",
    "\n",
    "# csv_path='./data_cut_only.csv'\n",
    "# vad_type = 'Valence'\n",
    "# data_cut = pd.read_csv('{}'.format(csv_path),encoding='utf-16')\n",
    "# data_cut = data_cut[data_cut['words']>=2]\n",
    "\n",
    "# X = data_cut['reg'].as_matrix()\n",
    "# Y = data_cut['{0}_reg'.format(vad_type)].as_matrix()\n",
    "\n",
    "# kf = KFold(n_splits=5,shuffle=True)\n",
    "\n",
    "# for train_index,test_index in kf.split(X):\n",
    "#     print(test_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
