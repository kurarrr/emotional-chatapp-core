{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data_preprocessed_cut_2_Valence.csv`と`data_preprocessed_cut_2_Arousal.csv`をtrainする  \n",
    "\n",
    "引数でdictを受け取るver\n",
    "devで相関係数を出す\n",
    "\n",
    "cross-validationをやる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.test.utils import datapath, get_tmpfile\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# # transform : glove -> tmp\n",
    "# glove2word2vec('./stanford_glove.txt', './stanford_w2v.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "gensim_model = gensim.models.KeyedVectors.load_word2vec_format('./stanford_w2v.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = gensim_model.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 2196016\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = weights.shape[1]\n",
    "print(embedding_dim,weights.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.append(weights,np.zeros((1,embedding_dim)),axis=0)\n",
    "# 末尾にunknown_wordを追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196017\n"
     ]
    }
   ],
   "source": [
    "vocab_size = weights.shape[0]\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "# cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # wordのindexを取得\n",
    "# print(gensim_model.wv.vocab['always'].index)\n",
    "# # 100番目のwordを取得\n",
    "# print(gensim_model.wv.index2word[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights[0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq):\n",
    "    vocab = gensim_model.wv.vocab\n",
    "    idxs = [vocab[w].index if w in vocab else vocab_size - 1 for w in seq]\n",
    "    res = torch.tensor(idxs, dtype=torch.long)\n",
    "    if cuda:\n",
    "        res = res.cuda()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vec(sentence):\n",
    "    w_list = sentence.split()\n",
    "    res_seq = prepare_sequence(w_list)\n",
    "    return res_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = \"I'm always fucking you.\"\n",
    "# sentence2vec(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, option, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = option['hidden_size']\n",
    "        self.num_layers = option['num_layers']\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.bi = (2 if option['bidirectional'] else 1)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        option['input_size'] = embedding_dim\n",
    "        option['batch_first'] = True\n",
    "\n",
    "        self.lstm = nn.LSTM(**option)\n",
    "        \n",
    "        # 2layer固定 調べるのめんどい\n",
    "        self.lstm.weight_hh_l0.data.uniform_(-0.01,0.01)        \n",
    "        self.lstm.weight_hh_l1.data.uniform_(-0.01,0.01)        \n",
    "        self.lstm.weight_ih_l0.data.uniform_(-0.01,0.01)        \n",
    "        self.lstm.weight_ih_l1.data.uniform_(-0.01,0.01)       \n",
    "        \n",
    "        self.lstm.weight_hh_l0_reverse.data.uniform_(-0.01,0.01)        \n",
    "        self.lstm.weight_hh_l1_reverse.data.uniform_(-0.01,0.01)        \n",
    "        self.lstm.weight_ih_l0_reverse.data.uniform_(-0.01,0.01)        \n",
    "        self.lstm.weight_ih_l1_reverse.data.uniform_(-0.01,0.01)       \n",
    "        \n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.out = nn.Linear(self.hidden_dim*self.bi, tagset_size)\n",
    "        self.out.weight.data.uniform_(-0.01,0.01)\n",
    "\n",
    "        self.hidden = self.init_hidden(1)\n",
    "\n",
    "    def init_hidden(self,batch_size):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        h = torch.zeros(self.num_layers*self.bi, batch_size, self.hidden_dim)\n",
    "        c = torch.zeros(self.num_layers*self.bi, batch_size, self.hidden_dim)\n",
    "        if cuda:\n",
    "            h = h.cuda()\n",
    "            c = c.cuda()\n",
    "        return (h,c)\n",
    "\n",
    "    def forward(self, sentence, lengths):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "#         print(embeds.size())\n",
    "#         batch_size = embeds.size()[0]\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embeds,lengths,batch_first=True)\n",
    "        lstm_output, self.hidden = self.lstm(packed, self.hidden)\n",
    "        unpacked,_ = nn.utils.rnn.pad_packed_sequence(lstm_output,batch_first=True)\n",
    "        # print(unpacked.size())\n",
    "        # :batch * max(len(lengths)) * hidden\n",
    "        \n",
    "        unpacked = torch.mean(unpacked,1)\n",
    "        # print(unpacked.size())\n",
    "        # :batch * hidden\n",
    "        output = self.out(unpacked)\n",
    "        output = F.tanh(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(option):\n",
    "    # 学習済みパラメータ\n",
    "    torch.manual_seed(2)\n",
    "    model = LSTMTagger(embedding_dim, option, vocab_size, out_size)\n",
    "    model.word_embeddings = nn.Embedding.from_pretrained(torch.from_numpy(weights).float())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h = torch.nn.LSTM(1,2,1)\n",
    "# h.weight_hh_l0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-68-4e8bc2901de5>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-68-4e8bc2901de5>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    )# ops = {\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# ops = {\n",
    "#     'hidden_size': 3,\n",
    "#     'num_layers' : 1,\n",
    "#     'bidirectional' : False,\n",
    "# }\n",
    "# model = make_model(ops)\n",
    "# bs = 4\n",
    "# model.zero_grad()\n",
    "# model.hidden = model.init_hidden(bs)\n",
    "# model(torch.randint(0,100,(bs,10),dtype=torch.long),[10 for _ in range(bs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,model_name):\n",
    "    model_state_dict = model.state_dict()\n",
    "    model_state_dict.pop('word_embeddings.weight')\n",
    "    torch.save(model_state_dict,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(hidden_dim,model_name):\n",
    "    torch.manual_seed(1)\n",
    "    model = LSTMTagger(embedding_dim, hidden_dim, vocab_size, out_size)\n",
    "    model_state_dict = torch.load(model_name)\n",
    "    model_state_dict['word_embeddings.weight'] = torch.from_numpy(weights).float()\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    # Freeze\n",
    "    model.word_embeddings.weight.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vad_type='Valence'\n",
    "# data_cut = pd.read_csv('./data_preprocessed_{0}.csv'.format(vad_type),encoding='utf-16')\n",
    "# data_cut = data_cut[data_cut['words']>=2]\n",
    "# X_train = data_cut[data_cut['data_type']=='train']['reg'].as_matrix()\n",
    "# Y_train = data_cut[data_cut['data_type']=='train'][['{0}_reg'.format(vad_type)]].as_matrix()\n",
    "# samples = []\n",
    "# for sentence,target in zip(X_train,Y_train):\n",
    "#     sentence_vec = sentence2vec(sentence)\n",
    "#     y_hat = torch.tensor(target, dtype=torch.float)\n",
    "#     if cuda:\n",
    "#         y_hat = y_hat.cuda()\n",
    "#     samples.append((sentence_vec,y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensor(vec, pad, dim):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        vec - tensor to pad\n",
    "        pad - the size to pad to\n",
    "        dim - dimension to pad\n",
    "\n",
    "    return:\n",
    "        a new tensor padded to 'pad' in dimension 'dim'\n",
    "    \"\"\"\n",
    "    pad_size = list(vec.shape)\n",
    "    pad_size[dim] = pad - vec.size(dim)\n",
    "    res = torch.cat([vec, (torch.zeros(*pad_size,dtype=torch.long).cuda() \\\n",
    "                           if cuda else torch.zeros(*pad_size,dtype=torch.long))], dim=dim)\n",
    "    if cuda:\n",
    "        res = res.cuda()\n",
    "    return res\n",
    "\n",
    "class PadCollate:\n",
    "    \"\"\"\n",
    "    a variant of callate_fn that pads according to the longest sequence in\n",
    "    a batch of sequences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim=0):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            dim - the dimension to be padded (dimension of time in sequences)\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "\n",
    "    def pad_collate(self, batch):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            batch - list of (tensor, label)\n",
    "\n",
    "        reutrn:\n",
    "            xs - a tensor of all examples in 'batch' after padding\n",
    "            ys - a LongTensor of all labels in batch\n",
    "        \"\"\"\n",
    "        lengths = list(map(lambda x: x[0].shape[self.dim], batch))\n",
    "        # find longest sequence\n",
    "        max_len = max(lengths)\n",
    "        # pad according to max_len\n",
    "        xs = torch.zeros([len(lengths),max_len],dtype=torch.long)\n",
    "        if cuda:\n",
    "            xs = xs.cuda()\n",
    "        for idx,(seq,seqlen) in enumerate(zip(batch,lengths)):\n",
    "            xs[idx,:seqlen] = seq[0]\n",
    "        ys = torch.FloatTensor(list(map(lambda x: x[1], batch)))\n",
    "        lengths_tensor = torch.tensor(lengths)\n",
    "\n",
    "        if cuda:\n",
    "            xs = xs.cuda()\n",
    "            ys = ys.cuda()\n",
    "            lengths_tensor = lengths_tensor.cuda()\n",
    "        return xs, ys, lengths_tensor\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return self.pad_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(samples,batch_size=4,shuffle=True,collate_fn=PadCollate(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sam in train_loader:\n",
    "#     sam\n",
    "#     print(sam[0].size(),sam[1].size())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def flatten(listOfLists):\n",
    "    \"Flatten one level of nesting\"\n",
    "    return list(chain.from_iterable(listOfLists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten([[1,2,3],[4,5,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(X,Y):\n",
    "    ds = []\n",
    "    for sentence,target in zip(X,Y):\n",
    "        sentence_vec = sentence2vec(sentence)\n",
    "        y_hat = torch.tensor(target, dtype=torch.float)\n",
    "        if cuda:\n",
    "            y_hat = y_hat.cuda()\n",
    "        ds.append((sentence_vec,y_hat))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_and_train_cross_validation_option(option,epochs,vad_type,csv_path='./data_cut_only.csv',metric='MSELoss',\n",
    "                         save_dir='./dat_model_json',learning_rate=0.01,batch_size=2,\n",
    "                         optimizer_name='Adam',print_result=True):\n",
    "    \n",
    "    # args\n",
    "    # option : lstmに渡すoption\n",
    "    # csv_path : csvへのパス\n",
    "    # save_dir : modelとjsonをsaveするdir (中にjson,modelディレクトリを含む)\n",
    "    \n",
    "    if not 'hidden_size' in option:\n",
    "        print('hidden_size is necessary')\n",
    "        return\n",
    "    hidden_dim = option['hidden_size']\n",
    "    \n",
    "    if not 'bidirectional' in option:\n",
    "        bidirectional = 0\n",
    "    else:\n",
    "        bidirectional = (1 if option['bidirectional'] else 0)\n",
    "    \n",
    "    if not 'num_layers' in option:\n",
    "        num_layers = 1\n",
    "    else:\n",
    "        num_layers = option['num_layers']\n",
    "        \n",
    "    if not 'dropout' in option:\n",
    "        dropout = 0\n",
    "    else:\n",
    "        dropout = option['dropout']\n",
    "    \n",
    "    json_name = './{0}./json/{1}_layer_{2}_bi_{3}_hd_{4}_bs_{5}_lr_{6}_dr_{7}_{8}.json'.format(\\\n",
    "                        save_dir,vad_type,num_layers,bidirectional,hidden_dim,batch_size,learning_rate,dropout,optimizer_name)\n",
    "    model_name = './{0}./model/{1}_layer_{2}_bi_{3}_hd_{4}_bs_{5}_lr_{6}_{7}'.format(\\\n",
    "                        save_dir,vad_type,num_layers,bidirectional,hidden_dim,batch_size,learning_rate,optimizer_name)\n",
    "\n",
    "\n",
    "    epoch_start = 0\n",
    "\n",
    "\n",
    "    data_cut = pd.read_csv('{}'.format(csv_path),encoding='utf-16')\n",
    "    data_cut = data_cut[data_cut['words']>=2]\n",
    "\n",
    "    X = data_cut['reg'].as_matrix()\n",
    "    Y = data_cut['{0}_reg'.format(vad_type)].as_matrix()\n",
    "\n",
    "\n",
    "    if metric == 'MSELoss':\n",
    "        loss_function = nn.MSELoss(size_average=False)\n",
    "    elif metric == 'L1Loss':\n",
    "        loss_function = nn.L1Loss(size_average=False)\n",
    "    else:\n",
    "        print('no loss funciton')\n",
    "        return\n",
    "    \n",
    "#     loss_function_metric = nn.L1Loss(size_average=False)\n",
    "    \n",
    "\n",
    "    # import time\n",
    "    # t1 = time.time()\n",
    "\n",
    "    train_losses = []\n",
    "    dev_losses = []\n",
    "    dev_coefs = []\n",
    "\n",
    "    n_splits = 5\n",
    "    kf = KFold(n_splits=n_splits,shuffle=True)\n",
    "    # 5-fold\n",
    "    k_split = 0\n",
    "    \n",
    "    best_cost = 10000 # INF\n",
    "    \n",
    "    for train_index,dev_index in kf.split(X):\n",
    "        k_split += 1\n",
    "        print(\"{} split / {}\".format(k_split,n_splits),flush=True)\n",
    "        \n",
    "        model = make_model(option)\n",
    "        \n",
    "        if cuda:\n",
    "            model.cuda()\n",
    "\n",
    "        if optimizer_name == 'Adadelta':\n",
    "            optimizer = optim.Adadelta(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "        elif optimizer_name == 'Adam':\n",
    "            optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "        elif optimizer_name == 'Adagrad':\n",
    "            optimizer = optim.Adagrad(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "        else:\n",
    "            print('{} couldnt be found'.format(optimizer_name))\n",
    "        \n",
    "        train_loss_part = []\n",
    "        dev_loss_part = []\n",
    "        dev_coef_part = []\n",
    "        \n",
    "        X_train, X_dev = X[train_index], X[dev_index]\n",
    "        Y_train, Y_dev = Y[train_index], Y[dev_index]\n",
    "        \n",
    "        ds_train = make_dataset(X_train,Y_train)\n",
    "        train_loader = DataLoader(ds_train,batch_size=batch_size,shuffle=True,collate_fn=PadCollate(dim=0))\n",
    "\n",
    "        ds_dev = make_dataset(X_dev,Y_dev)\n",
    "        dev_loader = DataLoader(ds_dev,batch_size=batch_size,shuffle=True,collate_fn=PadCollate(dim=0))\n",
    "\n",
    "        for epoch in range(epoch_start+1,epoch_start+1+epochs):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "            train_loss_sum = 0\n",
    "            \n",
    "            cnt = 0\n",
    "            for x_batch,y_batch,lengths in train_loader:\n",
    "                model.zero_grad()\n",
    "                model.hidden = model.init_hidden(len(x_batch))\n",
    "\n",
    "                # sort\n",
    "                lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "                x_batch = x_batch[perm_idx]\n",
    "                y_batch = y_batch[perm_idx]\n",
    "\n",
    "                y = model(x_batch,lengths)\n",
    "                y = y.view(-1)\n",
    "                loss = loss_function(y, y_batch)\n",
    "                \n",
    "                train_loss_sum += loss.data.item()\n",
    "                \n",
    "                # training Loss func\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                \n",
    "            train_loss_av = train_loss_sum/len(X_train)\n",
    "            train_loss_part.append(train_loss_av)            \n",
    "            \n",
    "            print(\"train : {} / {} = {}\".format(train_loss_sum,len(X_train),train_loss_av))\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            dev_loss_sum = 0\n",
    "\n",
    "            for x_batch,y_batch,lengths in dev_loader:\n",
    "                model.zero_grad()\n",
    "                model.hidden = model.init_hidden(len(x_batch))\n",
    "\n",
    "                # sort\n",
    "                lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "                x_batch = x_batch[perm_idx]\n",
    "                y_batch = y_batch[perm_idx]\n",
    "                y_true.append(list(y_batch.cpu().detach().numpy()))\n",
    "\n",
    "                y = model(x_batch,lengths)\n",
    "                y = y.view(-1)\n",
    "                y_pred.append(list(y.cpu().detach().numpy()))\n",
    "                loss = loss_function(y, y_batch)\n",
    "                dev_loss_sum += loss.data.item()\n",
    "\n",
    "            dev_loss_av = dev_loss_sum / len(X_dev)\n",
    "            \n",
    "            if dev_loss_av < best_cost:\n",
    "                save_model(model,model_name)\n",
    "                best_cost = dev_loss_av\n",
    "            \n",
    "            \n",
    "            dev_loss_part.append(dev_loss_av)\n",
    "            y_true = flatten(y_true)\n",
    "            y_pred = flatten(y_pred)\n",
    "            r = np.corrcoef(y_true,y_pred)[0,1]\n",
    "            dev_coef_part.append(r)\n",
    "                \n",
    "            print(\"dev : {} / {} = {}\".format(dev_loss_sum,len(X_dev),dev_loss_av))\n",
    "            print(\"r:    {}, size {}:{}\".format(r,len(y_true),len(y_pred)))\n",
    "            \n",
    "\n",
    "            # saveしない\n",
    "#             if epoch%10==0:\n",
    "#                 save_model(model,base_model_name+\"_epoch_{0}\".format(epoch))\n",
    "\n",
    "            if print_result:\n",
    "                print(\"epoch {0}: loss {1}\".format(epoch,train_loss_av),flush=True)\n",
    "            \n",
    "        train_losses.append(train_loss_part)\n",
    "        dev_losses.append(dev_loss_part)\n",
    "        dev_coefs.append(dev_coef_part)\n",
    "        \n",
    "        \n",
    "#     print(train_losses,dev_losses,dev_coefs)\n",
    "    train_loss = np.average(np.array(train_losses),axis=0).tolist()\n",
    "    dev_loss = np.average(np.array(dev_losses),axis=0).tolist()\n",
    "    dev_coef = np.average(np.array(dev_coefs),axis=0).tolist()\n",
    "    \n",
    "#     print(train_loss,dev_loss,dev_coef)\n",
    "#     print(train_losses)\n",
    "#     print(train_loss)\n",
    "#     print(dev_losses)\n",
    "#     print(dev_loss)\n",
    "    # t2 = time.time()\n",
    "\n",
    "    loss_data = {\n",
    "        'train' : train_loss,\n",
    "        'dev' : dev_loss,\n",
    "        'coef' : dev_coef\n",
    "    }\n",
    "    with open(json_name,'w') as f:\n",
    "        json.dump(loss_data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# op1 = {\n",
    "#     'hidden_size' : 60,\n",
    "#     'bidirectional' : True,   \n",
    "#     'num_layers' : 2,\n",
    "#     'dropout' : 0.5\n",
    "# }\n",
    "# # op2 = {\n",
    "# #     'hidden_size' : 32,\n",
    "# #     'bidirectional' : True,   \n",
    "# #     'num_layers' : 3,\n",
    "# # }\n",
    "# make_model_and_train_cross_validation_option(op1,1,'Valence',metric='L1Loss',csv_path='./data_cut_only.csv',\n",
    "#                             save_dir='./dat_model_json/valid/',\n",
    "#                             learning_rate=2e-5,batch_size=50,\n",
    "#                             optimizer_name='Adagrad')\n",
    "# # # make_model_and_train_option(op2,10,'Valence',metric='L1Loss',dat_base_name='./data_preprocessed_cut_2',\n",
    "# # #                             save_dir='./dat_model_json/dat_word_cut_l1loss_mullayer_bidirectional',\n",
    "# # #                             learning_rate=0.01,batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 split / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 1705.647777557373 / 4473 = 0.38132076404144266\n",
      "dev : 195.55743885040283 / 1119 = 0.17476089262770583\n",
      "2/5 0.0692222060700234, size 1119:1119\n",
      "train : 758.5383591651917 / 4473 = 0.16958156922986622\n",
      "dev : 177.5245189666748 / 1119 = 0.15864568272267632\n",
      "2/5 0.18971648648417838, size 1119:1119\n",
      "train : 716.0149049758911 / 4473 = 0.16007487256335595\n",
      "dev : 181.12547254562378 / 1119 = 0.16186369307026255\n",
      "2/5 0.3538393492257259, size 1119:1119\n",
      "train : 825.7634611129761 / 4473 = 0.1846106552901802\n",
      "dev : 168.52914929389954 / 1119 = 0.1506069251956207\n",
      "2/5 0.39085449158946023, size 1119:1119\n",
      "train : 781.9241268634796 / 4473 = 0.17480977573518436\n",
      "dev : 179.09567999839783 / 1119 = 0.16004975871170493\n",
      "2/5 0.41781236633089314, size 1119:1119\n",
      "train : 686.9208641052246 / 4473 = 0.15357050393588745\n",
      "dev : 174.0317578315735 / 1119 = 0.15552435909881454\n",
      "2/5 0.44251921805601135, size 1119:1119\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-a4db1d6fdae9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m                 make_model_and_train_cross_validation_option(option,epoch_num,vad_type,metric='L1Loss',csv_path='./data_cut_only.csv',\n\u001b[1;32m     42\u001b[0m                             \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./dat_model_json/dat_mean/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                             learning_rate=lr,batch_size=bs,print_result=False,optimizer_name='Adagrad')\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mcnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-40f43838f002>\u001b[0m in \u001b[0;36mmake_model_and_train_cross_validation_option\u001b[0;34m(option, epochs, vad_type, csv_path, metric, save_dir, learning_rate, batch_size, optimizer_name, print_result)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-da16fae73084>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentence, lengths)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m#         batch_size = embeds.size()[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mlstm_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0munpacked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# print(unpacked.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvariable_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight, batch_sizes)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# indexes of hiddens[0] before hiddens[1], while the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;31m# map will interleave them.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iteration over a 0-d tensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Adadeltaとdropout入れた\n",
    "# vad_types = ['Valence','Arousal']\n",
    "vad_types = ['Valence']\n",
    "# bss = [4]\n",
    "bss = [50]\n",
    "# Adadeltaは特にlearning rateを探索しなくて良い\n",
    "# lrs = [1e-3,1e-4,5e-5]\n",
    "lrs = [0.03,0.05,0.07]\n",
    "# for Adagrad\n",
    "# lrs = [0.5]\n",
    "# for Adadelta\n",
    "options = []\n",
    "\n",
    "hidden_dims = [180,240,300]\n",
    "num_layers = [2]\n",
    "bis = [True]\n",
    "drs = [0.5,0.25]\n",
    "\n",
    "for num_layer in num_layers:\n",
    "    for bi in bis:\n",
    "        for hidden_dim in hidden_dims:\n",
    "            for dr in drs:\n",
    "                options.append({\n",
    "                    'hidden_size' : hidden_dim,\n",
    "                    'bidirectional' : bi,   \n",
    "                    'num_layers' : num_layer,\n",
    "                    'dropout' : dr\n",
    "                })\n",
    "\n",
    "cnt = 0\n",
    "ma = len(vad_types)*len(lrs)*len(bss)*len(options)*len(drs)\n",
    "\n",
    "for vad_type in vad_types:\n",
    "    for lr in lrs:\n",
    "        for bs in bss:\n",
    "            for option in options:\n",
    "                epoch_num = 50\n",
    "#                 make_model_and_train_cross_validation_option(option,epoch_num,vad_type,metric='L1Loss',csv_path='./data_cut_only.csv',\n",
    "#                             save_dir='./dat_model_json/dat_stanford/',\n",
    "#                             learning_rate=lr,batch_size=bs,print_result=True,optimizer='Adadelta')\n",
    "                make_model_and_train_cross_validation_option(option,epoch_num,vad_type,metric='L1Loss',csv_path='./data_cut_only.csv',\n",
    "                            save_dir='./dat_model_json/valid/',\n",
    "                            learning_rate=lr,batch_size=bs,print_result=False,optimizer_name='Adagrad')\n",
    "                cnt += 1\n",
    "                print('{}/{}'.format(cnt,ma),flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Valenceの学習(確定版)\n",
    "# # Adadeltaとdropout入れた\n",
    "# # vad_types = ['Valence','Arousal']\n",
    "# vad_types = ['Valence']\n",
    "# # bss = [4]\n",
    "# bss = [50]\n",
    "# # Adadeltaは特にlearning rateを探索しなくて良い\n",
    "# # lrs = [1e-3,1e-4,5e-5]\n",
    "# lrs = [0.03,0.05,0.07]\n",
    "# # for Adagrad\n",
    "# # lrs = [0.5]\n",
    "# # for Adadelta\n",
    "# options = []\n",
    "\n",
    "# hidden_dims = [180,240,300]\n",
    "# num_layers = [2]\n",
    "# bis = [True]\n",
    "# drs = [0.5,0.25]\n",
    "\n",
    "# for num_layer in num_layers:\n",
    "#     for bi in bis:\n",
    "#         for hidden_dim in hidden_dims:\n",
    "#             for dr in drs:\n",
    "#                 options.append({\n",
    "#                     'hidden_size' : hidden_dim,\n",
    "#                     'bidirectional' : bi,   \n",
    "#                     'num_layers' : num_layer,\n",
    "#                     'dropout' : dr\n",
    "#                 })\n",
    "\n",
    "# cnt = 0\n",
    "# ma = len(vad_types)*len(lrs)*len(bss)*len(options)*len(drs)\n",
    "\n",
    "# for vad_type in vad_types:\n",
    "#     for lr in lrs:\n",
    "#         for bs in bss:\n",
    "#             for option in options:\n",
    "#                 epoch_num = 50\n",
    "# #                 make_model_and_train_cross_validation_option(option,epoch_num,vad_type,metric='L1Loss',csv_path='./data_cut_only.csv',\n",
    "# #                             save_dir='./dat_model_json/dat_stanford/',\n",
    "# #                             learning_rate=lr,batch_size=bs,print_result=True,optimizer='Adadelta')\n",
    "#                 make_model_and_train_cross_validation_option(option,epoch_num,vad_type,metric='L1Loss',csv_path='./data_cut_only.csv',\n",
    "#                             save_dir='./dat_model_json/valid/',\n",
    "#                             learning_rate=lr,batch_size=bs,print_result=False,optimizer_name='Adagrad')\n",
    "#                 cnt += 1\n",
    "#                 print('{}/{}'.format(cnt,ma),flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
