{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10358 10325 10142\n"
     ]
    }
   ],
   "source": [
    "# dataのload\n",
    "data_sentence = pd.read_table('./Emobank-master/corpus/raw.tsv')\n",
    "data_vad = pd.read_table('./Emobank-master/corpus/reader.tsv')\n",
    "data_merged = pd.merge(data_sentence,data_vad,on='id')\n",
    "print(len(data_sentence),len(data_vad),len(data_merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = data_merged['sentence']\n",
    "\n",
    "# 数字を全て0に\n",
    "p_num = re.compile(r'[0-9]+')\n",
    "ser = ser.apply(lambda x:p_num.sub(\"0\",x))\n",
    "\n",
    "# urlをremove \n",
    "p_url = re.compile(r'http[a-zA-Z0-9\\!\\#\\$\\%\\&\\'\\*\\+\\-\\.\\^_\\`\\|\\~\\:]*')\n",
    "ser = ser.apply(lambda x:p_url.sub(\" \",x))\n",
    "\n",
    "# replace .,! -> \\s\n",
    "ser = ser.apply(lambda x: x.replace(\".\",\" \").replace(\",\",\" \").replace(\"!\",\" \").replace(\"'\",\"\").replace(\"\\\"\",\"\").replace(\"“\",\"\").replace(\"”\",\"\"))\n",
    "\n",
    "# lowerに\n",
    "ser = ser.apply(lambda x:x.lower())\n",
    "\n",
    "data_preprocessed = data_merged.assign(\n",
    "    reg = ser\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "# sent = \"This is my text, this is a nice way  to input text. I'm angry\"\n",
    "# word_tokenize(sent)\n",
    "data_preprocessed = data_preprocessed.assign(\n",
    "    words = data_preprocessed.apply(lambda x: len(word_tokenize(x['reg'])), axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_preprocessed_2 is '\" -> space to '\" -> blank ver \n",
    "data_preprocessed.to_csv('data_preprocessed_2.csv',encoding=\"utf-16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>Dominance</th>\n",
       "      <th>Valence</th>\n",
       "      <th>sd.Arousal</th>\n",
       "      <th>sd.Dominance</th>\n",
       "      <th>sd.Valence</th>\n",
       "      <th>freq</th>\n",
       "      <th>reg</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acephalous-Cant-believe_4_47</td>\n",
       "      <td>I can't believe I wrote all that last year.</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>i can t believe i wrote all that last year</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acephalous-Cant-believe_83_354</td>\n",
       "      <td>Because I've been grading all damn day and am ...</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>5</td>\n",
       "      <td>because i ve been grading all damn day and am ...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acephalous-Cant-believe_355_499</td>\n",
       "      <td>However, when I started looking through my arc...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.489898</td>\n",
       "      <td>5</td>\n",
       "      <td>however  when i started looking through my arc...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acephalous-Cant-believe_500_515</td>\n",
       "      <td>What do I mean?</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>what do i mean?</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acephalous-Cant-believe_517_626</td>\n",
       "      <td>The posts I consider foundational to my curren...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>the posts i consider foundational to my curren...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id  \\\n",
       "0     Acephalous-Cant-believe_4_47   \n",
       "1   Acephalous-Cant-believe_83_354   \n",
       "2  Acephalous-Cant-believe_355_499   \n",
       "3  Acephalous-Cant-believe_500_515   \n",
       "4  Acephalous-Cant-believe_517_626   \n",
       "\n",
       "                                            sentence  Arousal  Dominance  \\\n",
       "0        I can't believe I wrote all that last year.      3.4        3.2   \n",
       "1  Because I've been grading all damn day and am ...      3.2        3.2   \n",
       "2  However, when I started looking through my arc...      3.0        3.2   \n",
       "3                                    What do I mean?      3.0        3.0   \n",
       "4  The posts I consider foundational to my curren...      3.0        3.0   \n",
       "\n",
       "   Valence  sd.Arousal  sd.Dominance  sd.Valence  freq  \\\n",
       "0      3.0         0.8           0.4    0.000000     5   \n",
       "1      2.8         0.4           0.4    0.400000     5   \n",
       "2      3.4         0.0           0.4    0.489898     5   \n",
       "3      3.0         0.0           0.0    0.000000     5   \n",
       "4      3.0         0.0           0.0    0.000000     5   \n",
       "\n",
       "                                                 reg  words  \n",
       "0        i can t believe i wrote all that last year      10  \n",
       "1  because i ve been grading all damn day and am ...     49  \n",
       "2  however  when i started looking through my arc...     26  \n",
       "3                                    what do i mean?      5  \n",
       "4  the posts i consider foundational to my curren...     18  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.620193255768093 -> 0.2461839530332681\n",
      "0.6566752119897457 -> 0.27669297881179894\n",
      "0.7308223230132124 -> 0.35185185185185186\n"
     ]
    }
   ],
   "source": [
    "col = ['Valence','Arousal','Dominance']\n",
    "\n",
    "for name in col:\n",
    "    rate1 = len(data_preprocessed[(data_preprocessed[name]>=2.8)&(data_preprocessed[name]<=3.2)])/len(data_preprocessed)\n",
    "    vad_mask = ((data_preprocessed[name]>=2.8)&(data_preprocessed[name]<=3.2))\n",
    "    data_1 = data_preprocessed[vad_mask]\n",
    "    data_2 = data_preprocessed[~(vad_mask)]\n",
    "    data_use,data_no_use = train_test_split(data_1, test_size=0.8)\n",
    "    data_cut = pd.concat([data_use,data_2])\n",
    "    rate2 = len(data_cut[(data_cut[name]>=2.8)&(data_cut[name]<=3.2)])/len(data_cut)\n",
    "    print(\"{0} -> {1}\".format(rate1,rate2))\n",
    "    train, dev_test = train_test_split(data_cut,test_size=0.4)\n",
    "    dev, test = train_test_split(dev_test,test_size=0.25)\n",
    "    # train/dev/test = 6/3/1\n",
    "    train = train.assign(data_type='train')\n",
    "    dev = dev.assign(data_type='dev')\n",
    "    test = test.assign(data_type='test')\n",
    "    data_cut = pd.concat([train,dev,test])\n",
    "    a=1\n",
    "    b=5\n",
    "    data_cut = data_cut.assign(\n",
    "        Valence_reg = data_cut.apply(lambda x: 2*(x['Valence']-a)/(b-a)-1, axis=1),\n",
    "        Arousal_reg = data_cut.apply(lambda x: 2*(x['Arousal']-a)/(b-a)-1, axis=1),\n",
    "        Dominance_reg = data_cut.apply(lambda x: 2*(x['Dominance']-a)/(b-a)-1, axis=1)\n",
    "    )\n",
    "    # [-1,1]で正規化\n",
    "    data_cut.to_csv('data_preprocessed_{0}.csv'.format(name),encoding='utf-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27669297881179894 0.5999169090153719\n"
     ]
    }
   ],
   "source": [
    "data_arousal = pd.read_csv('data_preprocessed_Arousal.csv',encoding='utf-16')\n",
    "rate_a = len(data_arousal[(data_arousal['Arousal']>=2.8)&(data_arousal['Arousal']<=3.2)])/len(data_arousal)\n",
    "rate_t = len(data_arousal[(data_arousal['data_type']=='train')])/len(data_arousal)\n",
    "\n",
    "print(rate_a,rate_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
