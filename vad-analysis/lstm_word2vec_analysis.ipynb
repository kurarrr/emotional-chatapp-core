{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "gensim_model = gensim.models.KeyedVectors.load_word2vec_format('./word2vec/data.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "weights = gensim_model.wv.syn0\n",
    "# weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = weights.shape[0]\n",
    "embedding_dim = weights.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.append(weights,np.zeros((1,embedding_dim)),axis=0)\n",
    "# 末尾にunknown_wordを追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = weights.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # wordのindexを取得\n",
    "# print(gensim_model.wv.vocab[\"'d\"].index)\n",
    "# # 100番目のwordを取得\n",
    "# print(gensim_model.wv.index2word[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq):\n",
    "    vocab = gensim_model.wv.vocab\n",
    "    idxs = [vocab[w].index if w in vocab else vocab_size - 1 for w in seq]\n",
    "    res = torch.tensor(idxs, dtype=torch.long)\n",
    "    if cuda:\n",
    "        res = res.cuda()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vec(sentence,debug=False):\n",
    "    w_list = word_tokenize(sentence)\n",
    "    w_list = [wordnet.morphy(w) if wordnet.morphy(w) is not None else w for w in w_list]\n",
    "    if debug:\n",
    "        print(w_list)\n",
    "    res_seq = prepare_sequence(w_list)\n",
    "    return res_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# s = \"I'm always fucking dogs.\"\n",
    "# sentence2vec(s,debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_and_train(hidden_dim,overwrite=False):\n",
    "    \n",
    "    class LSTMTagger(nn.Module):\n",
    "\n",
    "        def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "            super(LSTMTagger, self).__init__()\n",
    "            self.hidden_dim = hidden_dim\n",
    "\n",
    "            self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "            # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "            # with dimensionality hidden_dim.\n",
    "            self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "            # The linear layer that maps from hidden state space to tag space\n",
    "            self.out = nn.Linear(hidden_dim, tagset_size)\n",
    "            self.hidden = self.init_hidden()\n",
    "\n",
    "        def init_hidden(self):\n",
    "            # Before we've done anything, we dont have any hidden state.\n",
    "            # Refer to the Pytorch documentation to see exactly\n",
    "            # why they have this dimensionality.\n",
    "            # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "            h = torch.zeros(1, 1, self.hidden_dim)\n",
    "            c = torch.zeros(1, 1, self.hidden_dim)\n",
    "            if cuda:\n",
    "                h = h.cuda()\n",
    "                c = c.cuda()\n",
    "            return (h,c)\n",
    "\n",
    "        def forward(self, sentence):\n",
    "            embeds = self.word_embeddings(sentence)\n",
    "    #         print(embeds.size())\n",
    "\n",
    "            lstm_output, self.hidden = self.lstm(\n",
    "                embeds.view(len(sentence),1,-1), self.hidden)\n",
    "\n",
    "            output = self.out(lstm_output.view(len(sentence),-1))\n",
    "            output = F.tanh(output)\n",
    "            return output\n",
    "\n",
    "    out_size = 3\n",
    "    model = LSTMTagger(embedding_dim, hidden_dim, vocab_size, out_size)\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    json_name = './dat/loss_data_{0}.json'.format(hidden_dim)\n",
    "    model_name = './dat/model_data_{0}'.format(hidden_dim)\n",
    "    \n",
    "    if overwrite:\n",
    "        # 上書きする\n",
    "        model.load_state_dict(torch.load(model_name))\n",
    "        \n",
    "        with open(json_name,'r') as f:\n",
    "            dat = json.load(f)\n",
    "            \n",
    "        train_loss = dat['train']\n",
    "        dev_loss = dat['dev']\n",
    "    else:\n",
    "        # 学習済みパラメータ\n",
    "        pretrained_weights = torch.from_numpy(weights).float()\n",
    "        if cuda:\n",
    "            pretrained_weights = pretrained_weights.cuda()\n",
    "        model.word_embeddings = nn.Embedding.from_pretrained(pretrained_weights)\n",
    "        \n",
    "        train_loss = []\n",
    "        dev_loss = []\n",
    "\n",
    "        \n",
    "\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\n",
    "\n",
    "    # sample = [\n",
    "    #     \"I'm always fucking you hey\",\n",
    "    #     \"oh my godness\"\n",
    "    # ] \n",
    "\n",
    "    # # forwardのsample\n",
    "    # with torch.no_grad():\n",
    "    #     inputs = sentence2vec(sample[0])\n",
    "    #     output = model(inputs)\n",
    "    # #     print(output[-1,:])\n",
    "\n",
    "    # dataのload\n",
    "    data_pre = pd.read_csv('./data_preprocessed.csv')\n",
    "\n",
    "    # 2word以上のsentence\n",
    "    data_pre = data_pre[data_pre['words']>=2]\n",
    "\n",
    "    X_orig = data_pre['reg'].as_matrix()\n",
    "    Y_v = data_pre['Valence'].as_matrix()\n",
    "    Y_a = data_pre['Arousal'].as_matrix()\n",
    "    Y_d = data_pre['Dominance'].as_matrix()\n",
    "    Y_orig = np.c_[Y_v,Y_a,Y_d]\n",
    "    a=1\n",
    "    b=5\n",
    "    Y_orig = (2*(Y_orig-a)/(b-a))-1\n",
    "    # [-1,1]で正規化\n",
    "\n",
    "    X = X_orig\n",
    "    Y = Y_orig\n",
    "\n",
    "    train_size = 0.7\n",
    "    dev_size = 0.2\n",
    "    X_train, X_rest, Y_train, Y_rest = train_test_split(X, Y, test_size=1-train_size)\n",
    "    X_dev, X_test, Y_dev, Y_test = train_test_split(X_rest,Y_rest,test_size=1-(train_size+dev_size))\n",
    "\n",
    "    epochs = 20\n",
    "\n",
    "    # import time\n",
    "    # t1 = time.time()\n",
    "\n",
    "    for epoch in range(epochs):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "        train_loss_sum = 0\n",
    "        X_train,Y_train = shuffle(X_train,Y_train)\n",
    "        for sentence, target in zip(X_train,Y_train):\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Also, we need to clear out the hidden state of the LSTM,\n",
    "            # detaching it from its history on the last instance.\n",
    "            model.hidden = model.init_hidden()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "            sentence_in = sentence2vec(sentence)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            y = model(sentence_in)[-1,:]\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            y_hat = torch.tensor(target, dtype=torch.float)\n",
    "            if cuda:\n",
    "                y_hat = y_hat.cuda()\n",
    "            loss = loss_function(y, y_hat)\n",
    "            train_loss_sum += loss.data.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (epoch+1)%5==0:\n",
    "            dev_loss_sum = 0\n",
    "            for sentence, target in zip(X_dev,Y_dev):\n",
    "                # Step 1. Remember that Pytorch accumulates gradients.\n",
    "                # We need to clear them out before each instance\n",
    "                model.zero_grad()\n",
    "\n",
    "                # Also, we need to clear out the hidden state of the LSTM,\n",
    "                # detaching it from its history on the last instance.\n",
    "                model.hidden = model.init_hidden()\n",
    "\n",
    "                # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "                # Tensors of word indices.\n",
    "                sentence_in = sentence2vec(sentence)\n",
    "\n",
    "                # Step 3. Run our forward pass.\n",
    "                y = model(sentence_in)[-1,:]\n",
    "\n",
    "                # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "                #  calling optimizer.step()\n",
    "                y_hat = torch.tensor(target, dtype=torch.float)\n",
    "                if cuda:\n",
    "                    y_hat = y_hat.cuda()\n",
    "                loss = loss_function(y, y_hat)\n",
    "                dev_loss_sum += loss.data.item()\n",
    "\n",
    "            dev_loss_av = dev_loss_sum / len(X_dev)\n",
    "            dev_loss.append(dev_loss_av)\n",
    "\n",
    "        print(\"epoch {0}: loss {1}\".format(epoch,train_loss_sum/len(X_train)))\n",
    "\n",
    "\n",
    "        train_loss.append(train_loss_sum/len(X_train))\n",
    "\n",
    "    # t2 = time.time()\n",
    "\n",
    "    # print(t2-t1)\n",
    "\n",
    "    if True:\n",
    "        torch.save(model.state_dict(),model_name)\n",
    "\n",
    "#     print(train_loss)\n",
    "#     print(dev_loss)\n",
    "\n",
    "    loss_data = {\n",
    "        'train' : train_loss,\n",
    "        'dev' : dev_loss\n",
    "    }\n",
    "    with open(json_name,'w') as f:\n",
    "        json.dump(loss_data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss 0.03008627956157081\n",
      "epoch 1: loss 0.02993283518423051\n",
      "epoch 2: loss 0.029725415022920868\n",
      "epoch 3: loss 0.029517454050779556\n",
      "epoch 4: loss 0.029259214476232857\n",
      "epoch 5: loss 0.0290455570946983\n",
      "epoch 6: loss 0.028798799843558254\n",
      "epoch 7: loss 0.028534727643084783\n",
      "epoch 8: loss 0.028273659084371114\n",
      "epoch 9: loss 0.02802290279087233\n",
      "epoch 10: loss 0.02777865084692023\n",
      "epoch 11: loss 0.02750471652530416\n",
      "epoch 12: loss 0.027314357717259555\n",
      "epoch 13: loss 0.027108755559060893\n"
     ]
    }
   ],
   "source": [
    "hidden_dims = [6]\n",
    "for hidden_dim in hidden_dims:\n",
    "    make_model_and_train(hidden_dim,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
