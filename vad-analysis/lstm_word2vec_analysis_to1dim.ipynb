{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "gensim_model = gensim.models.KeyedVectors.load_word2vec_format('./word2vec/data.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "weights = gensim_model.wv.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = weights.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.append(weights,np.zeros((1,embedding_dim)),axis=0)\n",
    "# 末尾にunknown_wordを追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = weights.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wordのindexを取得\n",
    "# print(gensim_model.wv.vocab['always'].index)\n",
    "# 100番目のwordを取得\n",
    "# print(gensim_model.wv.index2word[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq):\n",
    "    vocab = gensim_model.wv.vocab\n",
    "    idxs = [vocab[w].index if w in vocab else vocab_size - 1 for w in seq]\n",
    "    res = torch.tensor(idxs, dtype=torch.long)\n",
    "    if cuda:\n",
    "        res = res.cuda()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vec(sentence):\n",
    "    w_list = sentence.split()\n",
    "    res_seq = prepare_sequence(w_list)\n",
    "    return res_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = \"I'm always fucking you.\"\n",
    "# sentence2vec(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.out = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        h = torch.zeros(1, 1, self.hidden_dim)\n",
    "        c = torch.zeros(1, 1, self.hidden_dim)\n",
    "        if cuda:\n",
    "            h = h.cuda()\n",
    "            c = c.cuda()\n",
    "        return (h,c)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "#         print(embeds.size())\n",
    "\n",
    "        lstm_output, self.hidden = self.lstm(\n",
    "            embeds.view(len(sentence),1,-1), self.hidden)\n",
    "\n",
    "        output = self.out(lstm_output.view(len(sentence),-1))\n",
    "        output = F.tanh(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(hidden_dim):\n",
    "    # 学習済みパラメータ\n",
    "    torch.manual_seed(1)\n",
    "    model = LSTMTagger(embedding_dim, hidden_dim, vocab_size, out_size)\n",
    "    model.word_embeddings = nn.Embedding.from_pretrained(torch.from_numpy(weights).float())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,model_name):\n",
    "    model_state_dict = model.state_dict()\n",
    "    model_state_dict.pop('word_embeddings.weight')\n",
    "    torch.save(model_state_dict,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(hidden_dim,model_name):\n",
    "    torch.manual_seed(1)\n",
    "    model = LSTMTagger(embedding_dim, hidden_dim, vocab_size, out_size)\n",
    "    model_state_dict = torch.load(model_name)\n",
    "    model_state_dict['word_embeddings.weight'] = torch.from_numpy(weights).float()\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    # Freeze\n",
    "    model.word_embeddings.weight.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_and_train(hidden_dim,epochs,vad_type,overwrite=False,model_name=\"\"):\n",
    "\n",
    "    json_name = './dat_to1/loss_{0}_hidden_dim_{1}.json'.format(vad_type,hidden_dim)\n",
    "    base_model_name = './dat_to1/model_{0}_hidden_dim_{1}'.format(vad_type,hidden_dim)\n",
    "\n",
    "    if not overwrite:\n",
    "        model = make_model(hidden_dim)\n",
    "        train_loss = []\n",
    "        dev_loss = []\n",
    "        epoch_start = 0\n",
    "    else:\n",
    "        model = load_model(hidden_dim,model_name)\n",
    "        with open(json_name,'r') as f:\n",
    "            dat = json.load(f)            \n",
    "        train_loss = dat['train']\n",
    "        dev_loss = dat['dev']\n",
    "        epoch_start = len(train_loss)\n",
    "\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    data_cut = pd.read_csv('./data_preprocessed_{0}.csv'.format(vad_type),encoding='utf-16')\n",
    "    data_cut = data_cut[data_cut['words']>=2]\n",
    "\n",
    "    X_train = data_cut[data_cut['data_type']=='train']['reg'].as_matrix()\n",
    "    X_dev   = data_cut[data_cut['data_type']=='dev']['reg'].as_matrix()\n",
    "    X_test  = data_cut[data_cut['data_type']=='test']['reg'].as_matrix()\n",
    "    Y_train = data_cut[data_cut['data_type']=='train'][['{0}_reg'.format(vad_type)]].as_matrix()\n",
    "    Y_dev   = data_cut[data_cut['data_type']=='dev'][['{0}_reg'.format(vad_type)]].as_matrix()\n",
    "    Y_test  = data_cut[data_cut['data_type']=='test'][['{0}_reg'.format(vad_type)]].as_matrix()\n",
    "\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\n",
    "\n",
    "    # import time\n",
    "    # t1 = time.time()\n",
    "\n",
    "    for epoch in range(epoch_start+1,epoch_start+1+epochs):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "        train_loss_sum = 0\n",
    "        X_train,Y_train = shuffle(X_train,Y_train)\n",
    "        for sentence, target in zip(X_train,Y_train):\n",
    "            model.zero_grad()\n",
    "            model.hidden = model.init_hidden()\n",
    "            sentence_in = sentence2vec(sentence)\n",
    "            y = model(sentence_in)[-1,:]\n",
    "            y_hat = torch.tensor(target, dtype=torch.float)\n",
    "            if cuda:\n",
    "                y_hat = y_hat.cuda()\n",
    "            loss = loss_function(y, y_hat)\n",
    "            train_loss_sum += loss.data.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (epoch+1)%5==0:\n",
    "            dev_loss_sum = 0\n",
    "            for sentence, target in zip(X_dev,Y_dev):\n",
    "                model.zero_grad()\n",
    "                model.hidden = model.init_hidden()\n",
    "                sentence_in = sentence2vec(sentence)\n",
    "                y = model(sentence_in)[-1,:]\n",
    "                y_hat = torch.tensor(target, dtype=torch.float)\n",
    "                if cuda:\n",
    "                    y_hat = y_hat.cuda()\n",
    "                loss = loss_function(y, y_hat)\n",
    "                dev_loss_sum += loss.data.item()\n",
    "\n",
    "            dev_loss_av = dev_loss_sum / len(X_dev)\n",
    "            dev_loss.append(dev_loss_av)\n",
    "          \n",
    "        if epoch%10==0:\n",
    "            # TODO: 上書きされる\n",
    "            save_model(model,base_model_name+\"_epoch_{0}\".format(epoch))\n",
    "        \n",
    "        train_loss_av = train_loss_sum/len(X_train)\n",
    "        print(\"epoch {0}: loss {1}\".format(epoch,train_loss_av))\n",
    "        train_loss.append(train_loss_av)\n",
    "\n",
    "    # t2 = time.time()\n",
    "\n",
    "    # print(t2-t1)\n",
    "\n",
    "    loss_data = {\n",
    "        'train' : train_loss,\n",
    "        'dev' : dev_loss\n",
    "    }\n",
    "    with open(json_name,'w') as f:\n",
    "        json.dump(loss_data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss 0.06678646548225617\n",
      "epoch 2: loss 0.06361196395907076\n",
      "epoch 1: loss 0.039991806882661275\n",
      "epoch 2: loss 0.03698048047259979\n",
      "epoch 1: loss 0.03195006386760086\n",
      "epoch 2: loss 0.028688972632514468\n"
     ]
    }
   ],
   "source": [
    "vad_types = ['Valence','Arousal','Dominance']\n",
    "hidden_dims = [4,8,16,32,64,128]\n",
    "for hidden_dim in hidden_dims:\n",
    "    for vad_type in vad_types:\n",
    "        epoch_num = 50\n",
    "        make_model_and_train(hidden_dim,epoch_num,vad_type,overwrite=False)\n",
    "#     make_model_and_train(hidden_dim,overwrite=True,model_name='./dat/model_data_2_epoch_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
