{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "gensim_model = gensim.models.KeyedVectors.load_word2vec_format('./word2vec/data.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "weights = gensim_model.wv.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = weights.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.append(weights,np.zeros((1,embedding_dim)),axis=0)\n",
    "# 末尾にunknown_wordを追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = weights.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wordのindexを取得\n",
    "# print(gensim_model.wv.vocab['always'].index)\n",
    "# 100番目のwordを取得\n",
    "# print(gensim_model.wv.index2word[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq):\n",
    "    vocab = gensim_model.wv.vocab\n",
    "    idxs = [vocab[w].index if w in vocab else vocab_size - 1 for w in seq]\n",
    "    res = torch.tensor(idxs, dtype=torch.long)\n",
    "    if cuda:\n",
    "        res = res.cuda()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vec(sentence):\n",
    "    w_list = sentence.split()\n",
    "    res_seq = prepare_sequence(w_list)\n",
    "    return res_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = \"I'm always fucking you.\"\n",
    "# sentence2vec(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.out = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden(1)\n",
    "\n",
    "    def init_hidden(self,batch_size):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        h = torch.zeros(1, batch_size, self.hidden_dim)\n",
    "        c = torch.zeros(1, batch_size, self.hidden_dim)\n",
    "        if cuda:\n",
    "            h = h.cuda()\n",
    "            c = c.cuda()\n",
    "        return (h,c)\n",
    "\n",
    "    def forward(self, sentence, lengths):\n",
    "        lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "        sentence = sentence[perm_idx]\n",
    "\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "#         print(embeds.size())\n",
    "#         batch_size = embeds.size()[0]\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embeds,lengths,batch_first=True)\n",
    "        lstm_output, self.hidden = self.lstm(packed, self.hidden)\n",
    "        unpacked,_ = nn.utils.rnn.pad_packed_sequence(lstm_output,batch_first=True)\n",
    "        # batch * hidden \n",
    "        output = self.out(unpacked[:,-1,:])\n",
    "        output = F.tanh(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(hidden_dim):\n",
    "    # 学習済みパラメータ\n",
    "    torch.manual_seed(1)\n",
    "    model = LSTMTagger(embedding_dim, hidden_dim, vocab_size, out_size)\n",
    "    model.word_embeddings = nn.Embedding.from_pretrained(torch.from_numpy(weights).float())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,model_name):\n",
    "    model_state_dict = model.state_dict()\n",
    "    model_state_dict.pop('word_embeddings.weight')\n",
    "    torch.save(model_state_dict,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(hidden_dim,model_name):\n",
    "    torch.manual_seed(1)\n",
    "    model = LSTMTagger(embedding_dim, hidden_dim, vocab_size, out_size)\n",
    "    model_state_dict = torch.load(model_name)\n",
    "    model_state_dict['word_embeddings.weight'] = torch.from_numpy(weights).float()\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    # Freeze\n",
    "    model.word_embeddings.weight.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vad_type='Valence'\n",
    "# data_cut = pd.read_csv('./data_preprocessed_{0}.csv'.format(vad_type),encoding='utf-16')\n",
    "# data_cut = data_cut[data_cut['words']>=2]\n",
    "# X_train = data_cut[data_cut['data_type']=='train']['reg'].as_matrix()\n",
    "# Y_train = data_cut[data_cut['data_type']=='train'][['{0}_reg'.format(vad_type)]].as_matrix()\n",
    "# samples = []\n",
    "# for sentence,target in zip(X_train,Y_train):\n",
    "#     sentence_vec = sentence2vec(sentence)\n",
    "#     y_hat = torch.tensor(target, dtype=torch.float)\n",
    "#     if cuda:\n",
    "#         y_hat = y_hat.cuda()\n",
    "#     samples.append((sentence_vec,y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensor(vec, pad, dim):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        vec - tensor to pad\n",
    "        pad - the size to pad to\n",
    "        dim - dimension to pad\n",
    "\n",
    "    return:\n",
    "        a new tensor padded to 'pad' in dimension 'dim'\n",
    "    \"\"\"\n",
    "    pad_size = list(vec.shape)\n",
    "    pad_size[dim] = pad - vec.size(dim)\n",
    "    res = torch.cat([vec, (torch.zeros(*pad_size,dtype=torch.long).cuda() \\\n",
    "                           if cuda else torch.zeros(*pad_size,dtype=torch.long))], dim=dim)\n",
    "    if cuda:\n",
    "        res = res.cuda()\n",
    "    return res\n",
    "\n",
    "class PadCollate:\n",
    "    \"\"\"\n",
    "    a variant of callate_fn that pads according to the longest sequence in\n",
    "    a batch of sequences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim=0):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            dim - the dimension to be padded (dimension of time in sequences)\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "\n",
    "    def pad_collate(self, batch):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            batch - list of (tensor, label)\n",
    "\n",
    "        reutrn:\n",
    "            xs - a tensor of all examples in 'batch' after padding\n",
    "            ys - a LongTensor of all labels in batch\n",
    "        \"\"\"\n",
    "        lengths = list(map(lambda x: x[0].shape[self.dim], batch))\n",
    "        # find longest sequence\n",
    "        max_len = max(lengths)\n",
    "        # pad according to max_len\n",
    "        xs = torch.zeros([len(lengths),max_len],dtype=torch.long)\n",
    "        if cuda:\n",
    "            xs = xs.cuda()\n",
    "        for idx,(seq,seqlen) in enumerate(zip(batch,lengths)):\n",
    "            xs[idx,:seqlen] = seq[0]\n",
    "        ys = torch.FloatTensor(list(map(lambda x: x[1], batch)))\n",
    "        lengths_tensor = torch.tensor(lengths)\n",
    "\n",
    "        if cuda:\n",
    "            xs = xs.cuda()\n",
    "            ys = ys.cuda()\n",
    "            lengths_tensor = lengths_tensor.cuda()\n",
    "        return xs, ys, lengths_tensor\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return self.pad_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(samples,batch_size=4,shuffle=True,collate_fn=PadCollate(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sam in train_loader:\n",
    "#     sam\n",
    "#     print(sam[0].size(),sam[1].size())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_and_train(hidden_dim,epochs,vad_type,learning_rate=0.01,batch_size=2,overwrite=False,model_name=\"\"):\n",
    "\n",
    "    json_name = './dat_to1_2/loss_{0}_hidden_dim_{1}_batch_{2}.json'.format(vad_type,hidden_dim,batch_size)\n",
    "    base_model_name = './dat_to1_2/model_{0}_hidden_dim_{1}_batch_{2}'.format(vad_type,hidden_dim,batch_size)\n",
    "\n",
    "    if not overwrite:\n",
    "        model = make_model(hidden_dim)\n",
    "        train_loss = []\n",
    "        dev_loss = []\n",
    "        epoch_start = 0\n",
    "    else:\n",
    "        model = load_model(hidden_dim,model_name)\n",
    "        with open(json_name,'r') as f:\n",
    "            dat = json.load(f)            \n",
    "        train_loss = dat['train']\n",
    "        dev_loss = dat['dev']\n",
    "        epoch_start = len(train_loss)\n",
    "\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    data_cut = pd.read_csv('./data_preprocessed_{0}.csv'.format(vad_type),encoding='utf-16')\n",
    "    data_cut = data_cut[data_cut['words']>=2]\n",
    "\n",
    "    X_train = data_cut[data_cut['data_type']=='train']['reg'].as_matrix()\n",
    "    X_dev   = data_cut[data_cut['data_type']=='dev']['reg'].as_matrix()\n",
    "    X_test  = data_cut[data_cut['data_type']=='test']['reg'].as_matrix()\n",
    "    Y_train = data_cut[data_cut['data_type']=='train'][['{0}_reg'.format(vad_type)]].as_matrix()\n",
    "    Y_dev   = data_cut[data_cut['data_type']=='dev'][['{0}_reg'.format(vad_type)]].as_matrix()\n",
    "    Y_test  = data_cut[data_cut['data_type']=='test'][['{0}_reg'.format(vad_type)]].as_matrix()\n",
    "\n",
    "    ds_train = []\n",
    "    for sentence,target in zip(X_train,Y_train):\n",
    "        sentence_vec = sentence2vec(sentence)\n",
    "        y_hat = torch.tensor(target, dtype=torch.float)\n",
    "        if cuda:\n",
    "            y_hat = y_hat.cuda()\n",
    "        ds_train.append((sentence_vec,y_hat))\n",
    "\n",
    "    train_loader = DataLoader(ds_train,batch_size=batch_size,shuffle=True,collate_fn=PadCollate(dim=0))\n",
    "\n",
    "    ds_dev = []\n",
    "    for sentence,target in zip(X_dev,Y_dev):\n",
    "        sentence_vec = sentence2vec(sentence)\n",
    "        y_hat = torch.tensor(target, dtype=torch.float)\n",
    "        if cuda:\n",
    "            y_hat = y_hat.cuda()\n",
    "        ds_dev.append((sentence_vec,y_hat))\n",
    "\n",
    "    dev_loader = DataLoader(ds_dev,batch_size=batch_size,shuffle=True,collate_fn=PadCollate(dim=0))\n",
    "\n",
    "        \n",
    "    loss_function = nn.L1Loss()\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "\n",
    "    # import time\n",
    "    # t1 = time.time()\n",
    "\n",
    "    for epoch in range(epoch_start+1,epoch_start+1+epochs):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "        train_loss_sum = 0\n",
    "        \n",
    "        for x_batch,y_batch,lengths in train_loader:\n",
    "            model.zero_grad()\n",
    "            model.hidden = model.init_hidden(len(x_batch))\n",
    "            y = model(x_batch,lengths)\n",
    "            y = y.view(-1)\n",
    "            loss = loss_function(y, y_batch)\n",
    "            train_loss_sum += loss.data.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (epoch+1)%5==0:\n",
    "            dev_loss_sum = 0\n",
    "            for x_batch,y_batch,lengths in dev_loader:\n",
    "                model.zero_grad()\n",
    "                model.hidden = model.init_hidden(len(x_batch))\n",
    "                y = model(x_batch,lengths)[-1,:]\n",
    "                y = y.view(-1)\n",
    "                loss = loss_function(y, y_batch)\n",
    "                dev_loss_sum += loss.data.item()\n",
    "\n",
    "            dev_loss_av = dev_loss_sum / len(X_dev)\n",
    "            dev_loss.append(dev_loss_av)\n",
    "          \n",
    "        if epoch%10==0:\n",
    "            save_model(model,base_model_name+\"_epoch_{0}\".format(epoch))\n",
    "        \n",
    "        train_loss_av = train_loss_sum/len(X_train)\n",
    "        print(\"epoch {0}: loss {1}\".format(epoch,train_loss_av))\n",
    "        train_loss.append(train_loss_av)\n",
    "\n",
    "    # t2 = time.time()\n",
    "\n",
    "    # print(t2-t1)\n",
    "\n",
    "    loss_data = {\n",
    "        'train' : train_loss,\n",
    "        'dev' : dev_loss\n",
    "    }\n",
    "    with open(json_name,'w') as f:\n",
    "        json.dump(loss_data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss 0.06266689832250084\n",
      "epoch 2: loss 0.06184785028690482\n",
      "epoch 3: loss 0.061665883467079484\n"
     ]
    }
   ],
   "source": [
    "make_model_and_train(3,3,'Valence',learning_rate=0.005,batch_size=4,overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_types = ['Valence','Arousal','Dominance']\n",
    "hidden_dims = [4,8,16,32,64,128]\n",
    "for hidden_dim in hidden_dims:\n",
    "    for vad_type in vad_types:\n",
    "        epoch_num = 50\n",
    "        make_model_and_train(hidden_dim,epoch_num,vad_type,batch_size=4,overwrite=False)\n",
    "#     make_model_and_train(hidden_dim,overwrite=True,model_name='./dat/model_data_2_epoch_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
