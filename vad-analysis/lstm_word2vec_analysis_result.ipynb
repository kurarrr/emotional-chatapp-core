{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "gensim_model = gensim.models.KeyedVectors.load_word2vec_format('./word2vec/data.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "weights = gensim_model.wv.syn0\n",
    "# weights.shape\n",
    "embedding_dim = weights.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.append(weights,np.zeros((1,embedding_dim)),axis=0)\n",
    "# 末尾にunknown_wordを追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = weights.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.out = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        h = torch.zeros(1, 1, self.hidden_dim)\n",
    "        c = torch.zeros(1, 1, self.hidden_dim)\n",
    "        if cuda:\n",
    "            h = h.cuda()\n",
    "            c = c.cuda()\n",
    "        return (h,c)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "#         print(embeds.size())\n",
    "\n",
    "        lstm_output, self.hidden = self.lstm(\n",
    "            embeds.view(len(sentence),1,-1), self.hidden)\n",
    "\n",
    "        output = self.out(lstm_output.view(len(sentence),-1))\n",
    "        output = F.tanh(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq):\n",
    "    vocab = gensim_model.wv.vocab\n",
    "    idxs = [vocab[w].index if w in vocab else vocab_size - 1 for w in seq]\n",
    "    res = torch.tensor(idxs, dtype=torch.long)\n",
    "    if cuda:\n",
    "        res = res.cuda()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vec(sentence,debug=False):\n",
    "    sentence = sentence.replace(\"'\",\" \").replace(\".\",\" \").replace(\",\",\" \").replace(\"\\\"\",\" \")\n",
    "    w_list = word_tokenize(sentence)\n",
    "    w_list = [wordnet.morphy(w).lower() if wordnet.morphy(w) is not None else w.lower() for w in w_list]\n",
    "    if debug:\n",
    "        print(w_list)\n",
    "    res_seq = prepare_sequence(w_list)\n",
    "    return res_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(hidden_dim,model_name):\n",
    "    torch.manual_seed(1)\n",
    "    model = LSTMTagger(embedding_dim, hidden_dim, vocab_size, out_size)\n",
    "    model_state_dict = torch.load(model_name)\n",
    "    model_state_dict['word_embeddings.weight'] = torch.from_numpy(weights).float()\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    model.word_embeddings.weight.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(hidden_dim,model_path,csv_name):\n",
    "    model = load_model(hidden_dim, model_path)\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    data_cut = pd.read_csv('./data_cut.csv',encoding='utf-16')\n",
    "    pred_list = []\n",
    "    for index,row in data_cut.iterrows():\n",
    "        sentence = row['sentence']\n",
    "        model.zero_grad()\n",
    "        model.hidden = model.init_hidden()\n",
    "        sentence_in = sentence2vec(sentence)\n",
    "        y = model(sentence_in)[-1,:]\n",
    "        if cuda:\n",
    "            y = y.cpu()\n",
    "        pred_list.append(y.detach().numpy())\n",
    "\n",
    "    pred_list = np.array(pred_list)\n",
    "\n",
    "    pred_list = pred_list*2 + 3\n",
    "\n",
    "    pred_data = data_cut.assign(\n",
    "        Valence_pred = pd.Series(pred_list[:,0]),\n",
    "        Arousal_pred = pd.Series(pred_list[:,1]),\n",
    "        Dominance_pred = pd.Series(pred_list[:,2])\n",
    "    )\n",
    "    pred_data.to_csv(csv_name,encoding='utf-16',sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 6\n",
    "model_path = './dat/model_data_6_epoch_59'\n",
    "csv_name = './dat/pred_6_epoch_59.csv'\n",
    "make_prediction(hidden_dim,model_path,csv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000001, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 6\n",
    "model_path = './dat/model_data_6_epoch_59'\n",
    "model = load_model(hidden_dim, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "in_dat = [\n",
    "    \"I feel so sorry about that\"\n",
    "]\n",
    "pred_list = []\n",
    "\n",
    "for i in in_dat:\n",
    "    model.zero_grad()\n",
    "    model.hidden = model.init_hidden()\n",
    "    sentence_in = sentence2vec(i)\n",
    "    y = model(sentence_in)[-1,:]\n",
    "    if cuda:\n",
    "        y = y.cpu()\n",
    "    pred_list.append(y.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = np.array(pred_list)\n",
    "pred_list = pred_list*2 + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Valence  ,Arousal  ,Dominance]\n",
      "[2.5193458 3.1973207 2.9886205]  I feel so sorry about that\n"
     ]
    }
   ],
   "source": [
    "pred_list = list(pred_list)\n",
    "print(\"[Valence  ,Arousal  ,Dominance]\")\n",
    "for sentence,vad in zip(in_dat,pred_list):\n",
    "    print(str(vad)+\"  \"+sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
